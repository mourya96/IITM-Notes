{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex #Imported for rendering of latex\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Import for generatring plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Recap\n",
    "1. Training data contains features and label that is a real number\n",
    "2. Model or inference: $\\bf y=Xw$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1: Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset of 100 examples with a single feature and a label.\n",
    "# For this conversation, we use the following three parameters\n",
    "w1 = 3\n",
    "w0 = 4\n",
    "n = 100\n",
    "\n",
    "x = 10*np.random.randn(n,)\n",
    "\n",
    "# Obtain y = 4 + 3*x + noise. Noise is randomly sampled.\n",
    "y = w0 + w1*x + np.random.randn(n,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets examine the shapes of the data for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data feature matrix: (100,)\n",
      "Shape of label vector: (100,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of training data feature matrix:', x.shape)\n",
    "print('Shape of label vector:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's divide the data into training and test set. We will set aside 20% examples for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick sanity check to make sure the sizes of features and labels sets are identical in both training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training feature matrix: (80,)\n",
      "Shape of training label vector: (80,)\n",
      "Shape of test feature matrix: (20,)\n",
      "Shape of test label matrix: (20,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of training feature matrix:', X_train.shape)\n",
    "print('Shape of training label vector:', y_train.shape)\n",
    "\n",
    "print('Shape of test feature matrix:', X_test.shape)\n",
    "print('Shape of test label matrix:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets quickly check the first few examples and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -3.70840327,  15.43552856,  13.96615884, -17.59543991,\n",
       "        -3.26141924])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -8.11853976,  50.80384663,  46.64428716, -51.38042402,\n",
       "        -6.50891188])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"white\")\n",
    "f = plt.figure(figsize=(8, 8))\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={'lines.linewidth': 2.5})\n",
    "\n",
    "plt.plot(X_train, y_train, \"b.\")\n",
    "plt.title(\"Data points\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0, 10, 0, 40])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a training set consisting of a single feature so we will fit a simple linear regression model with one feature. It's form is : $y=w_0+w_1x_1$\n",
    "\n",
    "As discussed in the lecture, we will add a special feature $x_0$ and set it to 1. We can create a helper function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dummy_feature(x):\n",
    "    ''' Adds a dummy feature to the dataset.\n",
    "    Args:\n",
    "        x: Training dataset\n",
    "    Returns:\n",
    "        Training dataset with an addition of dummy feature.\n",
    "    '''\n",
    "    # np.ones(X.shape[0]) create a vector of 1's having the same number of \n",
    "    # rows as number of samples in dataset.\n",
    "    return np.column_stack((np.ones(x.shape[0]),x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a test case to test this function:\n",
    "\n",
    "For that let's take a two examples and three features. The first example is a feature vector:\n",
    "$$\\textbf{x}_{3\\times1}^{(1)} = \\begin{bmatrix} 3 \\\\ 2 \\\\ 5 \\end{bmatrix}$$\n",
    "\n",
    "And the second example is:\n",
    "$$\\textbf{x}_{3\\times1}^{(2)} = \\begin{bmatrix} 9 \\\\ 4 \\\\ 7 \\end{bmatrix}$$\n",
    "\n",
    "And recall that a feature matrix $\\bf X$  has a shape *(n,m)* corresponding to features of all examples before adding the dummy feature $x_0$.\n",
    "$$\\textbf{X}_{n\\times m} = \\begin{bmatrix} {-(x^{(1)})^T-} \\\\ {-(x^{(2)})^T-} \\\\ . \\\\ . \\\\ . \\\\ {-(x^{(n)})^T-} \\end{bmatrix}$$\n",
    "\n",
    "In our current example, this becomes:\n",
    "$$\\textbf{X}_{2\\times3} = \\begin{bmatrix} {-(x^{(1)})^T-} \\\\ {-(x^{(2)})^T-} \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "The corresponding feature matrix $\\bf X$ appears as follows:\n",
    "$$\\textbf{X}_{2\\times3} = \\begin{bmatrix} 3 & 2 & 5 \\\\ 9 & 4 & 7 \\end{bmatrix}$$\n",
    "\n",
    "Here the feature vectors are transposed an represented as rows:\n",
    "- The first row corresponds to the first example $(\\textbf{x}^{(1)})^T$ and \n",
    "- The second row corresponds to the second example $(\\textbf{x}^{(2)})^T$.\n",
    "\n",
    "\n",
    "Once we add the dummy feature , the resulting matrix becomes:\n",
    "$$\\textbf{X}_{2\\times(3+1)} = \\begin{bmatrix} {-(x^{(1)})^T-} \\\\ {-(x^{(2)})^T-} \\end{bmatrix} = \\begin{bmatrix} 1 & 3 & 2 & 5 \\\\ 1 & 9 & 4 & 7 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_add_dummy_feature (__main__.TestAddDummyFeature.test_add_dummy_feature)\n",
      "Test case function for add_dummy_feature ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f60380e67b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestAddDummyFeature(unittest.TestCase):\n",
    "\n",
    "    def test_add_dummy_feature(self):\n",
    "        ''' Test case function for add_dummy_feature '''\n",
    "        train_matrix = np.array([[3, 4, 5],[9, 4, 7]])\n",
    "        train_matrix_with_dummy_feature = add_dummy_feature(train_matrix)\n",
    "\n",
    "        #Test the shape\n",
    "        self.assertEqual(train_matrix_with_dummy_feature.shape, (2,4))\n",
    "\n",
    "        #and the contents\n",
    "        np.testing.assert_array_equal(\n",
    "            train_matrix_with_dummy_feature, np.array([[1, 3, 4, 5],[1, 9, 4, 7]]))\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestAddDummyFeature', verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 3., 2.],\n",
       "       [1., 5., 4.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_dummy_feature(np.array([[3, 2],[5, 4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preprocess the training set to add the dummy feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before adding the dummy feature:\n",
      " [ -3.70840327  15.43552856  13.96615884 -17.59543991  -3.26141924]\n",
      "\n",
      "\n",
      "After adding the dummy feature:\n",
      " [[  1.          -3.70840327]\n",
      " [  1.          15.43552856]\n",
      " [  1.          13.96615884]\n",
      " [  1.         -17.59543991]\n",
      " [  1.          -3.26141924]]\n"
     ]
    }
   ],
   "source": [
    "print('Before adding the dummy feature:\\n', X_train[:5])\n",
    "print(\"\\n\")\n",
    "\n",
    "X_train_with_dummy = add_dummy_feature(X_train)\n",
    "print('After adding the dummy feature:\\n', X_train_with_dummy[:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Library import'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Library import'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Recap\n",
    "1. Training data contains features and label that is a real number\n",
    "2. Linear regression model uses linear combination of features to obtain output labels.In vectorized form Model or inference: $\\bf y=Xw$\n",
    "\n",
    "**Note**:\n",
    "- Model is paramterized by its weight vector.\n",
    "- It is described by its mathematical form and weight vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "The general vectorized form is as follows:\n",
    "$$\\textbf{y}_{(n\\times1)}=\\textbf{X}_{n\\times(m+1)}\\textbf{w}_{(m+1)\\times1}$$\n",
    "where\n",
    "- $n$ is the number of examples in dataset(train/test/validation).\n",
    "- $m$ is the number of features\n",
    "- $\\bf X$ is a feature matrix which contain $(m+1)$ features for $n$ examples along rows. (Notice capital case bold **X** used for matrix)\n",
    "- $\\bf w$ is a weight vector containing $(m+1)$ weights one for each feature. (Notice the small case bold **w**)\n",
    "- $\\bf y$ is a label vector containing labels of $n$ examples with shape $(n,)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,w):\n",
    "    ''' Prediction of output label for a given input.\n",
    "    Args:\n",
    "        X: Feature matrix of shape (n, m+1).\n",
    "        w: weight vector of shape (m+1, n).\n",
    "    Returns:\n",
    "        y: Predicted label vector of shape (n,).\n",
    "    '''\n",
    "    # Check to make sure that feature matrix and weight vector  have compatible shapes.\n",
    "    #print(X.shape,w.shape)\n",
    "    assert X.shape[-1] == w.shape[0]\n",
    "    return X @ w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test this function with the following feature matrix $\\textbf{X}_{2\\times(3+1)}$\n",
    "$$\\textbf{X}_{2\\times4} = \\begin{bmatrix}\n",
    "    1 & 3 & 2 & 5 \\\\ 1 & 9 & 4 & 7\n",
    "\\end{bmatrix}$$\n",
    "and the weight vector $\\bf w$\n",
    "$$\\textbf{w}_{4\\times1} = \\begin{bmatrix}\n",
    "    1 \\\\ 1 \\\\ 1 \\\\ 1    \n",
    "\\end{bmatrix}$$\n",
    "Let's perform a matrix vector multiplication between feature matrix $\\bf X$ and the weight vector $\\bf w$ to obtain labels for all examples:\n",
    "$$\\begin{align*}\n",
    "    \\textbf{y} \n",
    "    &= \\textbf{Xw} \\\\\n",
    "    &= \\begin{bmatrix} 1 & 3 & 2 & 5 \\\\ 1 & 9 & 4 & 7 \\end{bmatrix} \\times \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\\\\n",
    "    &= \\begin{bmatrix} {1\\times1+3\\times1+2\\times1+5\\times1} \\\\ {1\\times1+9\\times1+4\\times1+7\\times1} \\end{bmatrix} \\\\\n",
    "    &= \\begin{bmatrix} 11 \\\\ 21 \\end{bmatrix}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_predict (__main__.TestPredict.test_predict)\n",
      "Test case predict function of linear regression ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f6037d1f390>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestPredict(unittest.TestCase):\n",
    "\n",
    "    def test_predict(self):\n",
    "        ''' Test case predict function of linear regression '''\n",
    "        # set up\n",
    "        train_matrix = np.array([[1,3,2,5],[1,9,4,7]])\n",
    "        weight_vector = np.array([1,1,1,1])\n",
    "        expected_label_vector = np.array([11,21])\n",
    "\n",
    "        #call\n",
    "        predicted_label_vector = predict(train_matrix, weight_vector)\n",
    "\n",
    "        #asserts\n",
    "        #test the shape\n",
    "        self.assertEqual(predicted_label_vector.shape,(2,))\n",
    "\n",
    "        #and the contents\n",
    "        np.testing.assert_array_equal(\n",
    "            expected_label_vector,predicted_label_vector)\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestPredict', verbosity=2, exit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Preparing the dataset'''\n",
    "[w0,w1] = [4,3]\n",
    "n = 100\n",
    "x = 10*np.random.randn(n,)\n",
    "y = w0 + w1*x + np.random.randn(n,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training feature matrix: (80, 2)\n",
      "Shape of training label vector: (80,)\n",
      "Shape of test feature matrix: (20, 2)\n",
      "Shape of test label matrix: (20,)\n"
     ]
    }
   ],
   "source": [
    "''' Preprocessing: Dummy feature and train-test-split'''\n",
    "X_with_dummy = add_dummy_feature(x)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_with_dummy, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Shape of training feature matrix:', X_train.shape)\n",
    "print('Shape of training label vector:', y_train.shape)\n",
    "\n",
    "print('Shape of test feature matrix:', X_test.shape)\n",
    "print('Shape of test label matrix:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have not yet trained our model, let's use a random weight vector to get predictions from our model for a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5153733 , 0.20686153])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.random.rand(2,)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = predict(X_train,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the prediction with actual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.68230649, -2.50634755,  5.01262549,  1.07451807,  0.37188942,\n",
       "        1.57989027, -0.52234018,  0.91549391,  2.73709717,  1.36750572])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual labels are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-37.20249218, -17.11189712,  22.76186172,  48.44176238,\n",
       "        24.25142476, -10.95696155, -43.63897646,  -0.38976797,\n",
       "        37.32856002,  37.15101234])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we use a random weight vector $\\bf w$ here, most of the predicted labels do not match the actual labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision of vectorized and non-vectorized version of inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_vectorized_predict(X,w):\n",
    "    '''Prediction of output for a given input.\n",
    "    Args:\n",
    "        X: Feature matrix of shape (n, m+1)\n",
    "        w: Weight vector of shape (m+1, n)\n",
    "    Returns:\n",
    "        y: Predicted label vector of shape (n, ).\n",
    "    '''\n",
    "    y = []\n",
    "    for i in range(0,X.shape[0]):\n",
    "        y_hat_i = 0\n",
    "        for j in range(0,X.shape[1]):\n",
    "            y_hat_i += X[i][j]*w[j]\n",
    "        y.append(y_hat_i)\n",
    "    return np.array(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this function with the same setup as vectorized implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_predict_non_vectorized (__main__.TestPredictNonVectorized.test_predict_non_vectorized)\n",
      "Test case predict function of linear regression ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f6037d1f890>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestPredictNonVectorized(unittest.TestCase):\n",
    "\n",
    "    def test_predict_non_vectorized(self):\n",
    "        '''Test case predict function of linear regression '''\n",
    "        #set up\n",
    "        train_matrix = np.array([[1,3,2,5],[1,9,4,7]])\n",
    "        weight_vector = np.array([1,1,1,1])\n",
    "        expected_label_vector = np.array([11,21])\n",
    "\n",
    "        #call\n",
    "        predicted_label_vector = non_vectorized_predict(train_matrix,weight_vector)\n",
    "\n",
    "        #asserts\n",
    "        #test the shape\n",
    "        self.assertEqual(predicted_label_vector.shape, (2,))\n",
    "\n",
    "        #and its contents\n",
    "        np.testing.assert_array_equal(\n",
    "            expected_label_vector, predicted_label_vector)\n",
    "\n",
    "unittest.main(argv=[''],defaultTest='TestPredictNonVectorized', verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets compare run time of vectorized and non-vectorized versions on dataset with 100 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time incurred in the vectorized inference is 0.000088215 s\n",
      "Total time incurred in the non-vectorized inference is 0.000200510 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "y_hat_vectorized = predict(X_train,w)\n",
    "end_time = time.time()\n",
    "print('Total time incurred in the vectorized inference is %0.9f s'%(end_time-start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "y_hat_non_vectorized = non_vectorized_predict(X_train,w)\n",
    "end_time = time.time()\n",
    "print('Total time incurred in the non-vectorized inference is %0.9f s'%(end_time-start_time))\n",
    "\n",
    "np.testing.assert_array_equal(y_hat_vectorized,y_hat_non_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "977.0\n"
     ]
    }
   ],
   "source": [
    "def loss(X,y,w):\n",
    "    e  = predict(X,w) - y\n",
    "    return 0.5*(np.transpose(e) @ e)\n",
    "\n",
    "X=np.array([[1,2,2,1],[1,1,3,2]])\n",
    "y=np.array([3,5])\n",
    "w=np.array([1,2,3,4])*2\n",
    "a=loss(X,y,w)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal equation\n",
    "The weight vector is estimated by matrix multiplication of pseudo-inverse of feature matrix and label vector.\n",
    "\n",
    "The vectorized implementation is fairly straight forward.\n",
    "- We make use of ``np.linalg.pinv`` for calculating pseudo inverse of the feature matrix\n",
    "\n",
    "The equation is \n",
    "$$\n",
    "\\text{w} = (X^TX)^{-1} X^T y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation(X, y):\n",
    "    '''Estimates parameters of the linear regression model with normal equation\n",
    "    Args:\n",
    "        X: feature matrix for given inputs.\n",
    "        y: Actual label vector\n",
    "\n",
    "    Returns:\n",
    "        Weight vector\n",
    "    '''\n",
    "    return np.linalg.pinv(X) @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test this function with generated training set whose weight vector is known to us.\n",
    "- We setup the test with feature matrix, label vector and expected weight vectors\n",
    "- Next we estimate the weight vector with ``normal_equation`` function.\n",
    "- We test (a) shape and (ii) match between expected and estimated weight vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_normal_equation (__main__.TestNormalEquation.test_normal_equation)\n",
      "Test case for weight estimation for linear regression with normal equation method ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.006s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f6037ca5a70>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestNormalEquation(unittest.TestCase):\n",
    "\n",
    "    def test_normal_equation(self):\n",
    "        '''\n",
    "        Test case for weight estimation for linear regression with normal equation method\n",
    "        '''\n",
    "\n",
    "        # set up\n",
    "        feature_matrix = X_train\n",
    "        label_vector = y_train\n",
    "        expected_weight_vector = np.array([4.,3.])\n",
    "\n",
    "        #call\n",
    "        estimated_weight_vector = normal_equation(feature_matrix,label_vector)\n",
    "\n",
    "        #asserts\n",
    "        #test the shape\n",
    "        self.assertEqual(estimated_weight_vector.shape, (2,))\n",
    "\n",
    "        #and the contents\n",
    "        np.testing.assert_array_almost_equal(estimated_weight_vector, expected_weight_vector,decimal=0)\n",
    "\n",
    "unittest.main(argv=[''],defaultTest='TestNormalEquation', verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent(GD)\n",
    "GD is implemented as follows\n",
    "- Randomly initialize $w$ to 0.\n",
    "- Iterate until convergence:\n",
    "  - Calculate partial derivative of loss w.r.t weight vector\n",
    "  - Calculate new values of weights\n",
    "  - Update weights to new values simultaneously\n",
    "\n",
    "We use number of epochs as a convergence criteria in this implementation\n",
    "#### Partial derivative of loss function\n",
    "Let's first implement a function to calculate partial derivative of loss function, which is obtained with the following equation:\n",
    "$$\\bf \\frac{\\partial}{\\partial w}J(w) = X^T(Xw - y)$$\n",
    "The multiplication of transpose of feature matrix with the difference of predicted and actual label vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(X,y,w):\n",
    "    '''Calculate gradients of loss function w.r.t weight vector on training set\n",
    "    Arguments:\n",
    "        X: Feature matrix for training data\n",
    "        y: label vector of training data\n",
    "        w: Weight vector\n",
    "\n",
    "    Returns:\n",
    "        A vector of gradients\n",
    "    '''\n",
    "    return np.transpose(X) @ (predict(X,w) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(w, grad, lr):\n",
    "    ''' Updates the weights based on the gradient of the loss function.\n",
    "    Weight updates are carried out with the following formula:\n",
    "        w_new := w_old -lr*grad\n",
    "    Args:\n",
    "        1. w: weight vector\n",
    "        2. grad: gradient of loss w.r.t w\n",
    "        3. lr: learning rate\n",
    "    Returns:\n",
    "        Updated weight vector\n",
    "    '''\n",
    "    return (w - lr*grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X:np.ndarray, y:np.ndarray, lr:float, num_epochs:int):\n",
    "    '''Estimates parameters of linear regression model through gradient descent.\n",
    "    Args:\n",
    "        X: feature matrix for training data.\n",
    "        y: label vector for training data\n",
    "        lr: learning rate\n",
    "        num_epochs: Number of training steps\n",
    "    Returns:\n",
    "        Weight vector: Final weight vector\n",
    "        Error vector across different iterations\n",
    "        Weight vectors across different iterations\n",
    "    '''\n",
    "    w_all = [] # All parameters across different iterations\n",
    "    err_all = [] # All errors across different iterations\n",
    "    \n",
    "    # Parameter vector initialized to [0,0]\n",
    "    w = np.zeros(X.shape[1])\n",
    "    # gradient descent loop\n",
    "    print()\n",
    "    for i in np.arange(0,num_epochs):\n",
    "        w_all.append(w)\n",
    "        err_all.append(loss(X=X,w=w,y=y))\n",
    "        dJdw = calculate_gradient(X=X,y=y,w=w)\n",
    "        if (i%100) == 0:\n",
    "            print('Iteration #: %d, loss: %4.2f'%(i, err_all[-1]))\n",
    "        w = update_weights(w=w,grad=dJdw,lr=lr)\n",
    "    return w, err_all, w_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_gradient_descent (__main__.TestGradientDescent.test_gradient_descent)\n",
      "Test case for weight estimation for linear regression with gradient descent ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration #: 0, loss: 35754.19\n",
      "Iteration #: 100, loss: 164.31\n",
      "Iteration #: 200, loss: 60.00\n",
      "Iteration #: 300, loss: 38.94\n",
      "Iteration #: 400, loss: 34.69\n",
      "Iteration #: 500, loss: 33.83\n",
      "Iteration #: 600, loss: 33.66\n",
      "Iteration #: 700, loss: 33.62\n",
      "Iteration #: 800, loss: 33.61\n",
      "Iteration #: 900, loss: 33.61\n",
      "Iteration #: 1000, loss: 33.61\n",
      "Iteration #: 1100, loss: 33.61\n",
      "Iteration #: 1200, loss: 33.61\n",
      "Iteration #: 1300, loss: 33.61\n",
      "Iteration #: 1400, loss: 33.61\n",
      "Iteration #: 1500, loss: 33.61\n",
      "Iteration #: 1600, loss: 33.61\n",
      "Iteration #: 1700, loss: 33.61\n",
      "Iteration #: 1800, loss: 33.61\n",
      "Iteration #: 1900, loss: 33.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.018s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f6037ca4d60>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestGradientDescent(unittest.TestCase):\n",
    "\n",
    "    def test_gradient_descent(self):\n",
    "        '''\n",
    "        Test case for weight estimation for linear regression with gradient descent\n",
    "        '''\n",
    "\n",
    "        #setup\n",
    "        feature_matrix = X_train\n",
    "        label_vector = y_train\n",
    "        expected_weights = np.array([4.,3.])\n",
    "\n",
    "        #call\n",
    "        w, err_all, w_all = gradient_descent(\n",
    "            feature_matrix, label_vector, lr=0.0001, num_epochs=2000)\n",
    "        self.assertEqual(w.shape, (2,))\n",
    "        \n",
    "        np.testing.assert_array_almost_equal(expected_weights, w, decimal=0)\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestGradientDescent',verbosity=2,exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent (MBGD)\n",
    "The key idea is to perform weight updates by computing gradient on batches of small number of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0, t1 = 200, 100000\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gd(X:np.ndarray, y:np.ndarray, num_iters:int, minibatch_size:int):\n",
    "    w_all = []\n",
    "    err_all = []\n",
    "\n",
    "    w = np.zeros((X.shape[1]))\n",
    "    t=0\n",
    "\n",
    "    for epoch in range(num_iters):\n",
    "        shuffled_indices = np.random.permutation(X.shape[0])\n",
    "        X_shuffled = X[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            t += 1\n",
    "            xi = X_shuffled[i:i+minibatch_size]\n",
    "            yi = y_shuffled[i:i+minibatch_size]\n",
    "            #print(xi.shape[-1] == w.shape[0])\n",
    "            err_all.append(loss(xi,yi,w))\n",
    "            \n",
    "            gradients = 2/minibatch_size * calculate_gradient(xi, yi, w)\n",
    "            lr = learning_schedule(t)\n",
    "            w = update_weights(w, gradients, lr)\n",
    "            w_all.append(w)\n",
    "    return w, err_all, w_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_minibatch_gd (__main__.TestMiniBatchGradientDescent.test_minibatch_gd)\n",
      "Test case for weight estimation for linear regression with gradient descent ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.029s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f60380e17f0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestMiniBatchGradientDescent(unittest.TestCase):\n",
    "\n",
    "    def test_minibatch_gd(self):\n",
    "        '''\n",
    "        Test case for weight estimation for linear regression with gradient descent\n",
    "        '''\n",
    "\n",
    "        #setup\n",
    "        feature_matrix = X_train\n",
    "        label_vector = y_train\n",
    "        expected_weights = np.array([4.,3.])\n",
    "\n",
    "        #call\n",
    "        w, err_all, w_all = mini_batch_gd(\n",
    "            feature_matrix, label_vector, num_iters=200, minibatch_size=8)\n",
    "        self.assertEqual(w.shape, (2,))\n",
    "        \n",
    "        np.testing.assert_array_almost_equal(expected_weights, w, decimal=0)\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestMiniBatchGradientDescent',verbosity=2,exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)\n",
    "It is infact MBGD but with number of example per batch =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X:np.ndarray, y:np.ndarray, num_epochs:int):\n",
    "    w_all = []\n",
    "    err_all = []\n",
    "\n",
    "    w = np.zeros((X.shape[1]))\n",
    "    t=0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(X.shape[0]):\n",
    "            random_index = np.random.randint(X.shape[0])\n",
    "            xi = X[random_index:random_index+1]\n",
    "            yi = y[random_index:random_index+1]\n",
    "            err_all.append(loss(xi, yi, w))\n",
    "\n",
    "            gradients = 2 * calculate_gradient(xi, yi, w)\n",
    "            lr = learning_schedule(epoch * X.shape[0] + i)\n",
    "            w = update_weights(w, gradients, lr)\n",
    "            w_all.append(w)\n",
    "    return w, err_all, w_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_sgd (__main__.TestStochasticGradientDescent.test_sgd)\n",
      "Test case for weight estimation for linear regression with gradient descent ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.158s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f60380c5d00>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestStochasticGradientDescent(unittest.TestCase):\n",
    "\n",
    "    def test_sgd(self):\n",
    "        '''\n",
    "        Test case for weight estimation for linear regression with gradient descent\n",
    "        '''\n",
    "\n",
    "        #setup\n",
    "        feature_matrix = X_train\n",
    "        label_vector = y_train\n",
    "        expected_weights = np.array([4.,3.])\n",
    "\n",
    "        #call\n",
    "        w, err_all, w_all = sgd(\n",
    "            feature_matrix, label_vector, 200)\n",
    "        self.assertEqual(w.shape, (2,))\n",
    "        \n",
    "        np.testing.assert_array_almost_equal(expected_weights, w, decimal=0)\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestStochasticGradientDescent',verbosity=2,exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression: combining all components\n",
    "This part combines all the functions and components we implemented in the previous lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine implementations of different components of Linear Regression that we implemented so far into a single ``LinearRegression``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinReg(object):\n",
    "    '''\n",
    "    Linear Regression model\n",
    "    -----------------------\n",
    "    y = X @ w\n",
    "    X: A feature matrix\n",
    "    w: weight vector\n",
    "    y: label vector\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.t0 = 200\n",
    "        self.t1 = 100000\n",
    "    \n",
    "    def predict(self , X:np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Prediction of output label for a given input.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for given inputs.\n",
    "        Returns:\n",
    "            y: Output vector as predicted by the given model.\n",
    "        '''\n",
    "        y = x @ self.w\n",
    "        return y\n",
    "    \n",
    "    def loss(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        '''\n",
    "        Calculate the loss for a model based on known labels\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for given inputs.\n",
    "            y: Output label vector as predicted by the given model.\n",
    "        Returns:\n",
    "            Loss\n",
    "        '''\n",
    "\n",
    "        e = y - self.predict(X)\n",
    "        return 0.5* (np.transpose(e)) @ e\n",
    "    \n",
    "    def rmse (self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        '''\n",
    "        Calculate the root mean squared error of predictions w.r.t actual label.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for given inputs.\n",
    "            y: Output label vector as predicted by the given model\n",
    "        Returns:\n",
    "            Loss\n",
    "        '''\n",
    "        return np.sqrt((2/X.shape[0])) * self.loss(X,y)\n",
    "    \n",
    "\n",
    "    def fit (self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Estimates the parameters of the linear regression model with normal equation\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for given inputs.\n",
    "            y: Output label vector as predicted by the given model\n",
    "        Returns:\n",
    "            weight vector\n",
    "        '''\n",
    "        self.w = np.linalg.pinv(X) @ y\n",
    "        return self.w\n",
    "    \n",
    "\n",
    "    def calculate_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Calculates gradients of loss function w.r.t weight vector on training set.\n",
    "        Args:\n",
    "            X: Feature matrix for given inputs.\n",
    "            y: Output label vector as predicted by the given model\n",
    "        Returns:\n",
    "            a vector of gradients.\n",
    "        '''\n",
    "        return np.transpose(X) @ (self.predict(X) - y)\n",
    "    \n",
    "    def update_weights (self, grad:np.ndarray, lr: float) -> np.ndarray:\n",
    "        '''\n",
    "        Updates the weights based on the gradient of the loss function.\n",
    "\n",
    "        Weight updates are carried out with the following formula:\n",
    "            w_new := w_old - lr * grad\n",
    "        Args:\n",
    "            1. w: weight vector\n",
    "            2. grad: gradient of loss w.r.t w\n",
    "            3. lr: learning rate\n",
    "        Returns:\n",
    "            Updated weight vector\n",
    "        '''\n",
    "        return (self.w - lr * grad)\n",
    "\n",
    "    def learning_schedule(self, t):\n",
    "        return self.t0 / (t + self.t1)\n",
    "    \n",
    "    def gd(self, X: np.ndarray, y:np.ndarray, num_epochs:int, lr:float) -> np.ndarray:\n",
    "        '''\n",
    "        Estimates the parameters of linear regression model through gradient descent\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for training data\n",
    "            y: Label vector for training data\n",
    "            num_epochs: Number of training steps\n",
    "            lr: learning rate\n",
    "\n",
    "        Returns:\n",
    "            Weight vector: Final weight vector\n",
    "        '''\n",
    "        self.w = np.zeros((X.shape[1]))\n",
    "        self.w_all = []\n",
    "        self.err_all = []\n",
    "        for i in np.arange(0,num_epochs):\n",
    "            dJdw = self.calculate_gradient(X, y)\n",
    "            self.w_all.append(self.w)\n",
    "            self.err_all.append(self.loss(X,y))\n",
    "            self.w = self.update_weights(dJdw, lr)\n",
    "        return self.w\n",
    "    \n",
    "    def mbgd (self, X:np.ndarray, y:np.ndarray, num_epochs:int, batch_size:int) -> np.ndarray:\n",
    "        '''\n",
    "        Estimates the parameters of linear regression model through gradient descent\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for training data\n",
    "            y: Label vector for training data\n",
    "            num_epochs: Number of training steps\n",
    "            batch_size: Number of examples in a batch\n",
    "\n",
    "        Returns:\n",
    "            Weight vector: Final weight vector\n",
    "        '''\n",
    "        self.w = np.zeros((X.shape[1]))\n",
    "        self.w_all = []\n",
    "        self.err_all = []\n",
    "        mini_batch_id = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            shuffled_indices = np.random.permutation(X.shap[0])\n",
    "            X_shuffled = X[shuffled_indices]\n",
    "            y_shuffled = y[shuffled_indices]\n",
    "\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                mini_batch_id += 1\n",
    "                xi = X_shuffled[i: i+batch_size]\n",
    "                yi = y_shuffled[i: i+batch_size]\n",
    "                self.w_all.append(self.w)\n",
    "                self.err_all.append(self.loss(xi, yi))\n",
    "\n",
    "                dJdw = 2/batch_size * self.calculate_gradient(xi, yi)\n",
    "                self.w = self.update_weights(dJdw, self.learning_schedule(mini_batch_id))\n",
    "            \n",
    "        return self.w\n",
    "    \n",
    "    def sgd (self, X: np.ndarray, y:np.ndarray, num_epochs:int) -> np.ndarray:\n",
    "        '''\n",
    "        Estimates the parameters of linear regression model through gradient descent\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for training data\n",
    "            y: Label vector for training data\n",
    "            num_epochs: Number of training steps\n",
    "            batch_size: Number of examples in a batch\n",
    "\n",
    "        Returns:\n",
    "            Weight vector: Final weight vector\n",
    "        '''\n",
    "        self.w = np.zeros((X.shape[1]))\n",
    "        self.w_all = []\n",
    "        self.err_all = []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            for i in range(X.shape[0]):\n",
    "                random_index = np.random.randint(X.shape[0])\n",
    "                xi = X[random_index:random_index+1]\n",
    "                yi = y[random_index:random_index+1]\n",
    "\n",
    "                self.w_all.append(self.w)\n",
    "                self.err_all.append(self.loss(xi,yi))\n",
    "\n",
    "                gradients = 2 * self.calculate_gradient(xi, yi)\n",
    "                lr = self.learning_schedule(epoch * X.shape[0] +i)\n",
    "                self.w = self.update_weights(gradients, lr)\n",
    "        \n",
    "        return self.w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
