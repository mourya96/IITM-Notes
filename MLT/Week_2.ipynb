{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex #Imported for rendering of latex\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Import for generatring plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Recap\n",
    "1. Training data contains features and label that is a real number\n",
    "2. Model or inference: $\\bf y=Xw$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1: Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset of 100 examples with a single feature and a label.\n",
    "# For this conversation, we use the following three parameters\n",
    "w1 = 3\n",
    "w0 = 4\n",
    "n = 100\n",
    "\n",
    "x = 10*np.random.randn(n,)\n",
    "\n",
    "# Obtain y = 4 + 3*x + noise. Noise is randomly sampled.\n",
    "y = w0 + w1*x + np.random.randn(n,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets examine the shapes of the data for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data feature matrix: (100,)\n",
      "Shape of label vector: (100,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of training data feature matrix:', x.shape)\n",
    "print('Shape of label vector:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's divide the data into training and test set. We will set aside 20% examples for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick sanity check to make sure the sizes of features and labels sets are identical in both training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training feature matrix: (80,)\n",
      "Shape of training label vector: (80,)\n",
      "Shape of test feature matrix: (20,)\n",
      "Shape of test label matrix: (20,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of training feature matrix:', X_train.shape)\n",
    "print('Shape of training label vector:', y_train.shape)\n",
    "\n",
    "print('Shape of test feature matrix:', X_test.shape)\n",
    "print('Shape of test label matrix:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets quickly check the first few examples and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.34854881, -3.37767262, -4.51440968, -1.04558485,  3.34109256])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.22004564, -5.99969968, -8.73277447, -0.18040507, 14.79484836])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"white\")\n",
    "f = plt.figure(figsize=(8, 8))\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={'lines.linewidth': 2.5})\n",
    "\n",
    "plt.plot(X_train, y_train, \"b.\")\n",
    "plt.title(\"Data points\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0, 10, 0, 40])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a training set consisting of a single feature so we will fit a simple linear regression model with one feature. It's form is : $y=w_0+w_1x_1$\n",
    "\n",
    "As discussed in the lecture, we will add a special feature $x_0$ and set it to 1. We can create a helper function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dummy_feature(x):\n",
    "    ''' Adds a dummy feature to the dataset.\n",
    "    Args:\n",
    "        x: Training dataset\n",
    "    Returns:\n",
    "        Training dataset with an addition of dummy feature.\n",
    "    '''\n",
    "    # np.ones(X.shape[0]) create a vector of 1's having the same number of \n",
    "    # rows as number of samples in dataset.\n",
    "    return np.column_stack((np.ones(x.shape[0]),x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a test case to test this function:\n",
    "\n",
    "For that let's take a two examples and three features. The first example is a feature vector:\n",
    "$$\\textbf{x}_{3\\times1}^{(1)} = \\begin{bmatrix} 3 \\\\ 2 \\\\ 5 \\end{bmatrix}$$\n",
    "\n",
    "And the second example is:\n",
    "$$\\textbf{x}_{3\\times1}^{(2)} = \\begin{bmatrix} 9 \\\\ 4 \\\\ 7 \\end{bmatrix}$$\n",
    "\n",
    "And recall that a feature matrix $\\bf X$  has a shape *(n,m)* corresponding to features of all examples before adding the dummy feature $x_0$.\n",
    "$$\\textbf{X}_{n\\times m} = \\begin{bmatrix} {-(x^{(1)})^T-} \\\\ {-(x^{(2)})^T-} \\\\ . \\\\ . \\\\ . \\\\ {-(x^{(n)})^T-} \\end{bmatrix}$$\n",
    "\n",
    "In our current example, this becomes:\n",
    "$$\\textbf{X}_{2\\times3} = \\begin{bmatrix} {-(x^{(1)})^T-} \\\\ {-(x^{(2)})^T-} \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "The corresponding feature matrix $\\bf X$ appears as follows:\n",
    "$$\\textbf{X}_{2\\times3} = \\begin{bmatrix} 3 & 2 & 5 \\\\ 9 & 4 & 7 \\end{bmatrix}$$\n",
    "\n",
    "Here the feature vectors are transposed an represented as rows:\n",
    "- The first row corresponds to the first example $(\\textbf{x}^{(1)})^T$ and \n",
    "- The second row corresponds to the second example $(\\textbf{x}^{(2)})^T$.\n",
    "\n",
    "\n",
    "Once we add the dummy feature , the resulting matrix becomes:\n",
    "$$\\textbf{X}_{2\\times(3+1)} = \\begin{bmatrix} {-(x^{(1)})^T-} \\\\ {-(x^{(2)})^T-} \\end{bmatrix} = \\begin{bmatrix} 1 & 3 & 2 & 5 \\\\ 1 & 9 & 4 & 7 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_add_dummy_feature (__main__.TestAddDummyFeature)\n",
      "Test case function for add_dummy_feature ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f4a9b037d00>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestAddDummyFeature(unittest.TestCase):\n",
    "\n",
    "    def test_add_dummy_feature(self):\n",
    "        ''' Test case function for add_dummy_feature '''\n",
    "        train_matrix = np.array([[3, 4, 5],[9, 4, 7]])\n",
    "        train_matrix_with_dummy_feature = add_dummy_feature(train_matrix)\n",
    "\n",
    "        #Test the shape\n",
    "        self.assertEqual(train_matrix_with_dummy_feature.shape, (2,4))\n",
    "\n",
    "        #and the contents\n",
    "        np.testing.assert_array_equal(\n",
    "            train_matrix_with_dummy_feature, np.array([[1, 3, 4, 5],[1, 9, 4, 7]]))\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestAddDummyFeature', verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 3., 2.],\n",
       "       [1., 5., 4.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_dummy_feature(np.array([[3, 2],[5, 4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preprocess the training set to add the dummy feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before adding the dummy feature:\n",
      " [-3.34854881 -3.37767262 -4.51440968 -1.04558485  3.34109256]\n",
      "\n",
      "\n",
      "After adding the dummy feature:\n",
      " [[ 1.         -3.34854881]\n",
      " [ 1.         -3.37767262]\n",
      " [ 1.         -4.51440968]\n",
      " [ 1.         -1.04558485]\n",
      " [ 1.          3.34109256]]\n"
     ]
    }
   ],
   "source": [
    "print('Before adding the dummy feature:\\n', X_train[:5])\n",
    "print(\"\\n\")\n",
    "\n",
    "X_train_with_dummy = add_dummy_feature(X_train)\n",
    "print('After adding the dummy feature:\\n', X_train_with_dummy[:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Library import'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Library import'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Recap\n",
    "1. Training data contains features and label that is a real number\n",
    "2. Linear regression model uses linear combination of features to obtain output labels.In vectorized form Model or inference: $\\bf y=Xw$\n",
    "\n",
    "**Note**:\n",
    "- Model is paramterized by its weight vector.\n",
    "- It is described by its mathematical form and weight vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "The general vectorized form is as follows:\n",
    "$$\\textbf{y}_{(n\\times1)}=\\textbf{X}_{n\\times(m+1)}\\textbf{w}_{(m+1)\\times1}$$\n",
    "where\n",
    "- $n$ is the number of examples in dataset(train/test/validation).\n",
    "- $m$ is the number of features\n",
    "- $\\bf X$ is a feature matrix which contain $(m+1)$ features for $n$ examples along rows. (Notice capital case bold **X** used for matrix)\n",
    "- $\\bf w$ is a weight vector containing $(m+1)$ weights one for each feature. (Notice the small case bold **w**)\n",
    "- $\\bf y$ is a label vector containing labels of $n$ examples with shape $(n,)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,w):\n",
    "    ''' Prediction of output label for a given input.\n",
    "    Args:\n",
    "        X: Feature matrix of shape (n, m+1).\n",
    "        w: weight vector of shape (m+1, n).\n",
    "    Returns:\n",
    "        y: Predicted label vector of shape (n,).\n",
    "    '''\n",
    "    # Check to make sure that feature matrix and weight vector  have compatible shapes.\n",
    "    #print(X.shape,w.shape)\n",
    "    assert X.shape[-1] == w.shape[0]\n",
    "    return X @ w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test this function with the following feature matrix $\\textbf{X}_{2\\times(3+1)}$\n",
    "$$\\textbf{X}_{2\\times4} = \\begin{bmatrix}\n",
    "    1 & 3 & 2 & 5 \\\\ 1 & 9 & 4 & 7\n",
    "\\end{bmatrix}$$\n",
    "and the weight vector $\\bf w$\n",
    "$$\\textbf{w}_{4\\times1} = \\begin{bmatrix}\n",
    "    1 \\\\ 1 \\\\ 1 \\\\ 1    \n",
    "\\end{bmatrix}$$\n",
    "Let's perform a matrix vector multiplication between feature matrix $\\bf X$ and the weight vector $\\bf w$ to obtain labels for all examples:\n",
    "$$\\begin{align*}\n",
    "    \\textbf{y} \n",
    "    &= \\textbf{Xw} \\\\\n",
    "    &= \\begin{bmatrix} 1 & 3 & 2 & 5 \\\\ 1 & 9 & 4 & 7 \\end{bmatrix} \\times \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\\\\n",
    "    &= \\begin{bmatrix} {1\\times1+3\\times1+2\\times1+5\\times1} \\\\ {1\\times1+9\\times1+4\\times1+7\\times1} \\end{bmatrix} \\\\\n",
    "    &= \\begin{bmatrix} 11 \\\\ 21 \\end{bmatrix}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_predict (__main__.TestPredict)\n",
      "Test case predict function of linear regression ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f4a9b042ca0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestPredict(unittest.TestCase):\n",
    "\n",
    "    def test_predict(self):\n",
    "        ''' Test case predict function of linear regression '''\n",
    "        # set up\n",
    "        train_matrix = np.array([[1,3,2,5],[1,9,4,7]])\n",
    "        weight_vector = np.array([1,1,1,1])\n",
    "        expected_label_vector = np.array([11,21])\n",
    "\n",
    "        #call\n",
    "        predicted_label_vector = predict(train_matrix, weight_vector)\n",
    "\n",
    "        #asserts\n",
    "        #test the shape\n",
    "        self.assertEqual(predicted_label_vector.shape,(2,))\n",
    "\n",
    "        #and the contents\n",
    "        np.testing.assert_array_equal(\n",
    "            expected_label_vector,predicted_label_vector)\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestPredict', verbosity=2, exit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Preparing the dataset'''\n",
    "[w0,w1] = [4,3]\n",
    "n = 100\n",
    "x = 10*np.random.randn(n,)\n",
    "y = w0 + w1*x + np.random.randn(n,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training feature matrix: (80, 2)\n",
      "Shape of training label vector: (80,)\n",
      "Shape of test feature matrix: (20, 2)\n",
      "Shape of test label matrix: (20,)\n"
     ]
    }
   ],
   "source": [
    "''' Preprocessing: Dummy feature and train-test-split'''\n",
    "X_with_dummy = add_dummy_feature(x)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_with_dummy, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Shape of training feature matrix:', X_train.shape)\n",
    "print('Shape of training label vector:', y_train.shape)\n",
    "\n",
    "print('Shape of test feature matrix:', X_test.shape)\n",
    "print('Shape of test label matrix:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have not yet trained our model, let's use a random weight vector to get predictions from our model for a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89895308, 0.04647378])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.random.rand(2,)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = predict(X_train,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the prediction with actual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.13988231, 1.80409989, 0.80858219, 0.42863456, 0.83361928,\n",
       "       0.39458942, 0.72057461, 1.25365063, 1.44528051, 1.08061938])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual labels are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 29.41814809,  -8.6849606 ,  41.89470336,  16.46957661,\n",
       "        23.92187176, -18.52354947,  76.7279102 , -39.30318404,\n",
       "        34.89327557,  37.66510293])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we use a random weight vector $\\bf w$ here, most of the predicted labels do not match the actual labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision of vectorized and non-vectorized version of inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_vectorized_predict(X,w):\n",
    "    '''Prediction of output for a given input.\n",
    "    Args:\n",
    "        X: Feature matrix of shape (n, m+1)\n",
    "        w: Weight vector of shape (m+1, n)\n",
    "    Returns:\n",
    "        y: Predicted label vector of shape (n, ).\n",
    "    '''\n",
    "    y = []\n",
    "    for i in range(0,X.shape[0]):\n",
    "        y_hat_i = 0\n",
    "        for j in range(0,X.shape[1]):\n",
    "            y_hat_i += X[i][j]*w[j]\n",
    "        y.append(y_hat_i)\n",
    "    return np.array(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this function with the same setup as vectorized implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_predict_non_vectorized (__main__.TestPredictNonVectorized)\n",
      "Test case predict function of linear regression ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f4a9bb368e0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestPredictNonVectorized(unittest.TestCase):\n",
    "\n",
    "    def test_predict_non_vectorized(self):\n",
    "        '''Test case predict function of linear regression '''\n",
    "        #set up\n",
    "        train_matrix = np.array([[1,3,2,5],[1,9,4,7]])\n",
    "        weight_vector = np.array([1,1,1,1])\n",
    "        expected_label_vector = np.array([11,21])\n",
    "\n",
    "        #call\n",
    "        predicted_label_vector = non_vectorized_predict(train_matrix,weight_vector)\n",
    "\n",
    "        #asserts\n",
    "        #test the shape\n",
    "        self.assertEqual(predicted_label_vector.shape, (2,))\n",
    "\n",
    "        #and its contents\n",
    "        np.testing.assert_array_equal(\n",
    "            expected_label_vector, predicted_label_vector)\n",
    "\n",
    "unittest.main(argv=[''],defaultTest='TestPredictNonVectorized', verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets compare run time of vectorized and non-vectorized versions on dataset with 100 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time incurred in the vectorized inference is 0.000274420 s\n",
      "Total time incurred in the non-vectorized inference is 0.000319242 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "y_hat_vectorized = predict(X_train,w)\n",
    "end_time = time.time()\n",
    "print('Total time incurred in the vectorized inference is %0.9f s'%(end_time-start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "y_hat_non_vectorized = non_vectorized_predict(X_train,w)\n",
    "end_time = time.time()\n",
    "print('Total time incurred in the non-vectorized inference is %0.9f s'%(end_time-start_time))\n",
    "\n",
    "np.testing.assert_array_equal(y_hat_vectorized,y_hat_non_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "977.0\n"
     ]
    }
   ],
   "source": [
    "def loss(X,y,w):\n",
    "    e  = predict(X,w) - y\n",
    "    return 0.5*(np.transpose(e) @ e)\n",
    "\n",
    "X=np.array([[1,2,2,1],[1,1,3,2]])\n",
    "y=np.array([3,5])\n",
    "w=np.array([1,2,3,4])*2\n",
    "a=loss(X,y,w)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal equation\n",
    "The weight vector is estimated by matrix multiplication of pseudo-inverse of feature matrix and label vector.\n",
    "\n",
    "The vectorized implementation is fairly straight forward.\n",
    "- We make use of ``np.linalg.pinv`` for calculating pseudo inverse of the feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation(X, y):\n",
    "    '''Estimates parameters of the linear regression model with normal equation\n",
    "    Args:\n",
    "        X: feature matrix for given inputs.\n",
    "        y: Actual label vector\n",
    "\n",
    "    Returns:\n",
    "        Weight vector\n",
    "    '''\n",
    "    return np.linalg.pinv(X) @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test this function with generated training set whose weight vector is known to us.\n",
    "- We setup the test with feature matrix, label vector and expected weight vectors\n",
    "- Next we estimate the weight vector with ``normal_equation`` function.\n",
    "- We test (a) shape and (ii) match between expected and estimated weight vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_normal_equation (__main__.TestNormalEquation)\n",
      "Test case for weight estimation for linear regression with normal equation method ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.006s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f4a9afd15e0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestNormalEquation(unittest.TestCase):\n",
    "\n",
    "    def test_normal_equation(self):\n",
    "        '''\n",
    "        Test case for weight estimation for linear regression with normal equation method\n",
    "        '''\n",
    "\n",
    "        # set up\n",
    "        feature_matrix = X_train\n",
    "        label_vector = y_train\n",
    "        expected_weight_vector = np.array([4.,3.])\n",
    "\n",
    "        #call\n",
    "        estimated_weight_vector = normal_equation(feature_matrix,label_vector)\n",
    "\n",
    "        #asserts\n",
    "        #test the shape\n",
    "        self.assertEqual(estimated_weight_vector.shape, (2,))\n",
    "\n",
    "        #and the contents\n",
    "        np.testing.assert_array_almost_equal(estimated_weight_vector, expected_weight_vector,decimal=0)\n",
    "\n",
    "unittest.main(argv=[''],defaultTest='TestNormalEquation', verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent(GD)\n",
    "GD is implemented as follows\n",
    "- Randomly initialize $w$ to 0.\n",
    "- Iterate until convergence:\n",
    "  - Calculate partial derivative of loss w.r.t weight vector\n",
    "  - Calculate new values of weights\n",
    "  - Update weights to new values simultaneously\n",
    "\n",
    "We use number of epochs as a convergence criteria in this implementation\n",
    "#### Partial derivative of loss function\n",
    "Let's first implement a function to calculate partial derivative of loss function, which is obtained with the following equation:\n",
    "$$\\bf \\frac{\\partial}{\\partial w}J(w) = X^T(Xw - y)$$\n",
    "The multiplication of transpose of feature matrix with the difference of predicted and actual label vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(X,y,w):\n",
    "    '''Calculate gradients of loss function w.r.t weight vector on training set\n",
    "    Arguments:\n",
    "        X: Feature matrix for training data\n",
    "        y: label vector of training data\n",
    "        w: Weight vector\n",
    "\n",
    "    Returns:\n",
    "        A vector of gradients\n",
    "    '''\n",
    "    return np.transpose(X) @ (predict(X,w) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(w, grad, lr):\n",
    "    ''' Updates the weights based on the gradient of the loss function.\n",
    "    Weight updates are carried out with the following formula:\n",
    "        w_new := w_old -lr*grad\n",
    "    Args:\n",
    "        1. w: weight vector\n",
    "        2. grad: gradient of loss w.r.t w\n",
    "        3. lr: learning rate\n",
    "    Returns:\n",
    "        Updated weight vector\n",
    "    '''\n",
    "    return (w - lr*grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X:np.ndarray, y:np.ndarray, lr:float, num_epochs:int):\n",
    "    '''Estimates parameters of linear regression model through gradient descent.\n",
    "    Args:\n",
    "        X: feature matrix for training data.\n",
    "        y: label vector for training data\n",
    "        lr: learning rate\n",
    "        num_epochs: Number of training steps\n",
    "    Returns:\n",
    "        Weight vector: Final weight vector\n",
    "        Error vector across different iterations\n",
    "        Weight vectors across different iterations\n",
    "    '''\n",
    "    w_all = [] # All parameters across different iterations\n",
    "    err_all = [] # All errors across different iterations\n",
    "    \n",
    "    # Parameter vector initialized to [0,0]\n",
    "    w = np.zeros(X.shape[1])\n",
    "    # gradient descent loop\n",
    "    print()\n",
    "    for i in np.arange(0,num_epochs):\n",
    "        w_all.append(w)\n",
    "        err_all.append(loss(X=X,w=w,y=y))\n",
    "        dJdw = calculate_gradient(X=X,y=y,w=w)\n",
    "        if (i%100) == 0:\n",
    "            print('Iteration #: %d, loss: %4.2f'%(i, err_all[-1]))\n",
    "        w = update_weights(w=w,grad=dJdw,lr=lr)\n",
    "    return w, err_all, w_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_gradient_descent (__main__.TestGradientDescent)\n",
      "Test case for weight estimation for linear regression with gradient descent ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration #: 0, loss: 41852.70\n",
      "Iteration #: 100, loss: 180.15\n",
      "Iteration #: 200, loss: 76.59\n",
      "Iteration #: 300, loss: 54.98\n",
      "Iteration #: 400, loss: 50.47\n",
      "Iteration #: 500, loss: 49.53\n",
      "Iteration #: 600, loss: 49.34\n",
      "Iteration #: 700, loss: 49.30\n",
      "Iteration #: 800, loss: 49.29\n",
      "Iteration #: 900, loss: 49.29\n",
      "Iteration #: 1000, loss: 49.29\n",
      "Iteration #: 1100, loss: 49.29\n",
      "Iteration #: 1200, loss: 49.29\n",
      "Iteration #: 1300, loss: 49.29\n",
      "Iteration #: 1400, loss: 49.29\n",
      "Iteration #: 1500, loss: 49.29\n",
      "Iteration #: 1600, loss: 49.29\n",
      "Iteration #: 1700, loss: 49.29\n",
      "Iteration #: 1800, loss: 49.29\n",
      "Iteration #: 1900, loss: 49.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.123s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f4a9bb36d60>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestGradientDescent(unittest.TestCase):\n",
    "\n",
    "    def test_gradient_descent(self):\n",
    "        '''\n",
    "        Test case for weight estimation for linear regression with gradient descent\n",
    "        '''\n",
    "\n",
    "        #setup\n",
    "        feature_matrix = X_train\n",
    "        label_vector = y_train\n",
    "        expected_weights = np.array([4.,3.])\n",
    "\n",
    "        #call\n",
    "        w, err_all, w_all = gradient_descent(\n",
    "            feature_matrix, label_vector, lr=0.0001, num_epochs=2000)\n",
    "        self.assertEqual(w.shape, (2,))\n",
    "        \n",
    "        np.testing.assert_array_almost_equal(expected_weights, w, decimal=0)\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestGradientDescent',verbosity=2,exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent (MBGD)\n",
    "The key idea is to perform weight updates by computing gradient on batches of small number of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0, t1 = 200, 100000\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gd(X:np.ndarray, y:np.ndarray, num_iters:int, minibatch_size:int):\n",
    "    w_all = []\n",
    "    err_all = []\n",
    "\n",
    "    w = np.zeros((X.shape[1]))\n",
    "    t=0\n",
    "\n",
    "    for epoch in range(num_iters):\n",
    "        shuffled_indices = np.random.permutation(X.shape[0])\n",
    "        X_shuffled = X[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            t += 1\n",
    "            xi = X_shuffled[i:i+minibatch_size]\n",
    "            yi = y_shuffled[i:i+minibatch_size]\n",
    "            #print(xi.shape[-1] == w.shape[0])\n",
    "            err_all.append(loss(xi,yi,w))\n",
    "            \n",
    "            gradients = 2/minibatch_size * calculate_gradient(xi, yi, w)\n",
    "            lr = learning_schedule(t)\n",
    "            w = update_weights(w, gradients, lr)\n",
    "            w_all.append(w)\n",
    "    return w, err_all, w_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_minibatch_gd (__main__.TestMiniBatchGradientDescent)\n",
      "Test case for weight estimation for linear regression with gradient descent ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.104s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f4a9bb364f0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestMiniBatchGradientDescent(unittest.TestCase):\n",
    "\n",
    "    def test_minibatch_gd(self):\n",
    "        '''\n",
    "        Test case for weight estimation for linear regression with gradient descent\n",
    "        '''\n",
    "\n",
    "        #setup\n",
    "        feature_matrix = X_train\n",
    "        label_vector = y_train\n",
    "        expected_weights = np.array([4.,3.])\n",
    "\n",
    "        #call\n",
    "        w, err_all, w_all = mini_batch_gd(\n",
    "            feature_matrix, label_vector, num_iters=200, minibatch_size=8)\n",
    "        self.assertEqual(w.shape, (2,))\n",
    "        \n",
    "        np.testing.assert_array_almost_equal(expected_weights, w, decimal=0)\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestMiniBatchGradientDescent',verbosity=2,exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)\n",
    "It is infact MBGD but with number of example per batch =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X:np.ndarray, y:np.ndarray, num_epochs:int):\n",
    "    w_all = []\n",
    "    err_all = []\n",
    "\n",
    "    w = np.zeros((X.shape[1]))\n",
    "    t=0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(X.shape[0]):\n",
    "            random_index = np.random.randint(X.shape[0])\n",
    "            xi = X[random_index:random_index+1]\n",
    "            yi = y[random_index:random_index+1]\n",
    "            err_all.append(loss(xi, yi, w))\n",
    "\n",
    "            gradients = 2 * calculate_gradient(xi, yi, w)\n",
    "            lr = learning_schedule(epoch * X.shape[0] + i)\n",
    "            w = update_weights(w, gradients, lr)\n",
    "            w_all.append(w)\n",
    "    return w, err_all, w_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_sgd (__main__.TestStochasticGradientDescent)\n",
      "Test case for weight estimation for linear regression with gradient descent ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.607s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f4a9afe57c0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestStochasticGradientDescent(unittest.TestCase):\n",
    "\n",
    "    def test_sgd(self):\n",
    "        '''\n",
    "        Test case for weight estimation for linear regression with gradient descent\n",
    "        '''\n",
    "\n",
    "        #setup\n",
    "        feature_matrix = X_train\n",
    "        label_vector = y_train\n",
    "        expected_weights = np.array([4.,3.])\n",
    "\n",
    "        #call\n",
    "        w, err_all, w_all = sgd(\n",
    "            feature_matrix, label_vector, 200)\n",
    "        self.assertEqual(w.shape, (2,))\n",
    "        \n",
    "        np.testing.assert_array_almost_equal(expected_weights, w, decimal=0)\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestStochasticGradientDescent',verbosity=2,exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression: combining all components\n",
    "This part combines all the functions and components we implemented in the previous lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine implementations of different components of Linear Regression that we implemented so far into a single ``LinearRegression``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinReg(object):\n",
    "    '''\n",
    "    Linear Regression model\n",
    "    -----------------------\n",
    "    y = X @ w\n",
    "    X: A feature matrix\n",
    "    w: weight vector\n",
    "    y: label vector\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.t0 = 200\n",
    "        self.t1 = 100000\n",
    "    \n",
    "    def predict(self , X:np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Prediction of output label for a given input.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for given inputs.\n",
    "        Returns:\n",
    "            y: Output vector as predicted by the given model.\n",
    "        '''\n",
    "        y = x @ self.w\n",
    "        return y\n",
    "    \n",
    "    def loss(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        '''\n",
    "        Calculate the loss for a model based on known labels\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for given inputs.\n",
    "            y: Output label vector as predicted by the given model.\n",
    "        Returns:\n",
    "            Loss\n",
    "        '''\n",
    "\n",
    "        e = y - self.predict(X)\n",
    "        return 0.5* (np.transpose(e)) @ e\n",
    "    \n",
    "    def rmse (self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        '''\n",
    "        Calculate the root mean squared error of predictions w.r.t actual label.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for given inputs.\n",
    "            y: Output label vector as predicted by the given model\n",
    "        Returns:\n",
    "            Loss\n",
    "        '''\n",
    "        return np.sqrt((2/X.shape[0])) * self.loss(X,y)\n",
    "    \n",
    "\n",
    "    def fit (self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Estimates the parameters of the linear regression model with normal equation\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for given inputs.\n",
    "            y: Output label vector as predicted by the given model\n",
    "        Returns:\n",
    "            weight vector\n",
    "        '''\n",
    "        self.w = np.linalg.pinv(X) @ y\n",
    "        return self.w\n",
    "    \n",
    "\n",
    "    def calculate_gradient(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Calculates gradients of loss function w.r.t weight vector on training set.\n",
    "        Args:\n",
    "            X: Feature matrix for given inputs.\n",
    "            y: Output label vector as predicted by the given model\n",
    "        Returns:\n",
    "            a vector of gradients.\n",
    "        '''\n",
    "        return np.transpose(X) @ (self.predict(X) - y)\n",
    "    \n",
    "    def update_weights (self, grad:np.ndarray, lr: float) -> np.ndarray:\n",
    "        '''\n",
    "        Updates the weights based on the gradient of the loss function.\n",
    "\n",
    "        Weight updates are carried out with the following formula:\n",
    "            w_new := w_old - lr * grad\n",
    "        Args:\n",
    "            1. w: weight vector\n",
    "            2. grad: gradient of loss w.r.t w\n",
    "            3. lr: learning rate\n",
    "        Returns:\n",
    "            Updated weight vector\n",
    "        '''\n",
    "        return (self.w - lr * grad)\n",
    "\n",
    "    def learning_schedule(self, t):\n",
    "        return self.t0 / (t + self.t1)\n",
    "    \n",
    "    def gd(self, X: np.ndarray, y:np.ndarray, num_epochs:int, lr:float) -> np.ndarray:\n",
    "        '''\n",
    "        Estimates the parameters of linear regression model through gradient descent\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for training data\n",
    "            y: Label vector for training data\n",
    "            num_epochs: Number of training steps\n",
    "            lr: learning rate\n",
    "\n",
    "        Returns:\n",
    "            Weight vector: Final weight vector\n",
    "        '''\n",
    "        self.w = np.zeros((X.shape[1]))\n",
    "        self.w_all = []\n",
    "        self.err_all = []\n",
    "        for i in np.arange(0,num_epochs):\n",
    "            dJdw = self.calculate_gradient(X, y)\n",
    "            self.w_all.append(self.w)\n",
    "            self.err_all.append(self.loss(X,y))\n",
    "            self.w = self.update_weights(dJdw, lr)\n",
    "        return self.w\n",
    "    \n",
    "    def mbgd (self, X:np.ndarray, y:np.ndarray, num_epochs:int, batch_size:int) -> np.ndarray:\n",
    "        '''\n",
    "        Estimates the parameters of linear regression model through gradient descent\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for training data\n",
    "            y: Label vector for training data\n",
    "            num_epochs: Number of training steps\n",
    "            batch_size: Number of examples in a batch\n",
    "\n",
    "        Returns:\n",
    "            Weight vector: Final weight vector\n",
    "        '''\n",
    "        self.w = np.zeros((X.shape[1]))\n",
    "        self.w_all = []\n",
    "        self.err_all = []\n",
    "        mini_batch_id = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            shuffled_indices = np.random.permutation(X.shap[0])\n",
    "            X_shuffled = X[shuffled_indices]\n",
    "            y_shuffled = y[shuffled_indices]\n",
    "\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                mini_batch_id += 1\n",
    "                xi = X_shuffled[i: i+batch_size]\n",
    "                yi = y_shuffled[i: i+batch_size]\n",
    "                self.w_all.append(self.w)\n",
    "                self.err_all.append(self.loss(xi, yi))\n",
    "\n",
    "                dJdw = 2/batch_size * self.calculate_gradient(xi, yi)\n",
    "                self.w = self.update_weights(dJdw, self.learning_schedule(mini_batch_id))\n",
    "            \n",
    "        return self.w\n",
    "    \n",
    "    def sgd (self, X: np.ndarray, y:np.ndarray, num_epochs:int) -> np.ndarray:\n",
    "        '''\n",
    "        Estimates the parameters of linear regression model through gradient descent\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for training data\n",
    "            y: Label vector for training data\n",
    "            num_epochs: Number of training steps\n",
    "            batch_size: Number of examples in a batch\n",
    "\n",
    "        Returns:\n",
    "            Weight vector: Final weight vector\n",
    "        '''\n",
    "        self.w = np.zeros((X.shape[1]))\n",
    "        self.w_all = []\n",
    "        self.err_all = []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            for i in range(X.shape[0]):\n",
    "                random_index = np.random.randint(X.shape[0])\n",
    "                xi = X[random_index:random_index+1]\n",
    "                yi = y[random_index:random_index+1]\n",
    "\n",
    "                self.w_all.append(self.w)\n",
    "                self.err_all.append(self.loss(xi,yi))\n",
    "\n",
    "                gradients = 2 * self.calculate_gradient(xi, yi)\n",
    "                lr = self.learning_schedule(epoch * X.shape[0] +i)\n",
    "                self.w = self.update_weights(gradients, lr)\n",
    "        \n",
    "        return self.w"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9244b6adea22edad6e19cdea93c196ea7ddff3c1d91dfb077ea542e13d85dd05"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
