{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57aaf778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a01fbb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.random.seed(180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efc2350",
   "metadata": {},
   "source": [
    "### Quick Recap\n",
    "1. Training data contains features and label that is a real number\n",
    "2. Model or inference: $\\bf y=Xw$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dafdf62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset of 100 examples with a single feature and a label.\n",
    "# For this conversation, we use the following three parameters\n",
    "w1 = 3\n",
    "w0 = 4\n",
    "n = 100\n",
    "\n",
    "x = 10*cp.random.randn(n,)\n",
    "\n",
    "# Obtain y = 4 + 3*x + noise. Noise is randomly sampled.\n",
    "y = w0 + w1*x + cp.random.randn(n,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eb368b",
   "metadata": {},
   "source": [
    "Lets examine the shapes of the data for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3832ec28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data feature matrix:  (100,)\n",
      "Shape of target vector: (100,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of training data feature matrix: \", x.shape)\n",
    "print(\"Shape of target vector:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80da71c6",
   "metadata": {},
   "source": [
    "Let's divide the data into training and test set. We will set aside 20% of the examples for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec6e5d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e28d0",
   "metadata": {},
   "source": [
    "Let's do a quick sanity check to make sure the sizes of features and target sets are identical in both training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b02fe6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training feature matrix: (80,)\n",
      "Shape of training label vector: (80,)\n",
      "Shape of test feature matrix: (20,)\n",
      "Shape of test label matrix: (20,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of training feature matrix:', X_train.shape)\n",
    "print('Shape of training label vector:', y_train.shape)\n",
    "\n",
    "print('Shape of test feature matrix:', X_test.shape)\n",
    "print('Shape of test label matrix:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62649693",
   "metadata": {},
   "source": [
    "Let's quickly check the first few examples and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b83ea711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -8.35364802, -12.14206044,  -6.56057812, -15.23653087,\n",
       "         1.73798644])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 17.45645391,  43.5217927 , -25.36421206,  23.29849409,\n",
       "        27.5744419 ])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train[:5])\n",
    "display(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc260bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"white\")\n",
    "f = plt.figure(figsize=(8, 8))\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={'lines.linewidth': 2.5})\n",
    "\n",
    "plt.plot(X_train.get(), y_train.get(), \"b.\") # Converting cupy array to numpy array\n",
    "plt.title(\"Data points\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0, 10, 0, 40])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fd658f",
   "metadata": {},
   "source": [
    "We have a training set consisting of a single feature so we will fit a simple linear regression model with one feature. It's form is : $y=w_0+w_1x_1$\n",
    "\n",
    "As discussed in the lecture, we will add a special feature $x_0$ and set it to 1. We can create a helper function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8350d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dummy_feature(x):\n",
    "    ''' Adds a dummy feature to the dataset.\n",
    "    Args:\n",
    "        x: Training dataset\n",
    "    Returns:\n",
    "        Training dataset with an addition of dummy feature.\n",
    "    '''\n",
    "    # cp.ones(X.shape[0]) create a vector of 1's having the same number of \n",
    "    # rows as number of samples in dataset.\n",
    "    return cp.column_stack((cp.ones(x.shape[0]), x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaeddcb",
   "metadata": {},
   "source": [
    "Let's write a test case to test this function:\n",
    "\n",
    "For that let's take a two examples and three features. The first example is a feature vector:\n",
    "$$\\textbf{x}_{3\\times1}^{(1)} = \\begin{bmatrix} 3 \\\\ 2 \\\\ 5 \\end{bmatrix}$$\n",
    "\n",
    "And the second example is:\n",
    "$$\\textbf{x}_{3\\times1}^{(2)} = \\begin{bmatrix} 9 \\\\ 4 \\\\ 7 \\end{bmatrix}$$\n",
    "\n",
    "And recall that a feature matrix $\\bf X$  has a shape *(n,m)* corresponding to features of all examples before adding the dummy feature $x_0$.\n",
    "$$\\textbf{X}_{n\\times m} = \\begin{bmatrix} {-(x^{(1)})^T-} \\\\ {-(x^{(2)})^T-} \\\\ . \\\\ . \\\\ . \\\\ {-(x^{(n)})^T-} \\end{bmatrix}$$\n",
    "\n",
    "In our current example, this becomes:\n",
    "$$\\textbf{X}_{2\\times3} = \\begin{bmatrix} {-(x^{(1)})^T-} \\\\ {-(x^{(2)})^T-} \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "The corresponding feature matrix $\\bf X$ appears as follows:\n",
    "$$\\textbf{X}_{2\\times3} = \\begin{bmatrix} 3 & 2 & 5 \\\\ 9 & 4 & 7 \\end{bmatrix}$$\n",
    "\n",
    "Here the feature vectors are transposed an represented as rows:\n",
    "- The first row corresponds to the first example $(\\textbf{x}^{(1)})^T$ and \n",
    "- The second row corresponds to the second example $(\\textbf{x}^{(2)})^T$.\n",
    "\n",
    "\n",
    "Once we add the dummy feature , the resulting matrix becomes:\n",
    "$$\\textbf{X}_{2\\times(3+1)} = \\begin{bmatrix} {-(x^{(1)})^T-} \\\\ {-(x^{(2)})^T-} \\end{bmatrix} = \\begin{bmatrix} 1 & 3 & 2 & 5 \\\\ 1 & 9 & 4 & 7 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "891577bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_add_dummy_feature (__main__.TestAddDummyFeature.test_add_dummy_feature)\n",
      "Test case function for add_dummy_feature ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.291s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f26ddc781a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestAddDummyFeature(unittest.TestCase):\n",
    "\n",
    "    def test_add_dummy_feature(self):\n",
    "        \"\"\"Test case function for add_dummy_feature\"\"\"\n",
    "        train_matrix = cp.array([[3, 4, 5], [9, 4, 7]])\n",
    "        train_matrix_with_dummy_feature = add_dummy_feature(train_matrix)\n",
    "\n",
    "        #Test the shape\n",
    "        self.assertEqual(train_matrix_with_dummy_feature.shape, (2,4))\n",
    "\n",
    "        #and the contents\n",
    "        cp.testing.assert_array_equal(train_matrix_with_dummy_feature, cp.array([[1, 3, 4, 5],[1, 9, 4, 7]]))\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestAddDummyFeature', verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "602be3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 3., 2.],\n",
       "       [1., 5., 4.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_dummy_feature(cp.array([[3, 2],[5, 4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdda88e3",
   "metadata": {},
   "source": [
    "Let's preprocess the training set to add the dummy feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6344d307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before adding the dummy feature:\n",
      " [ -8.35364802 -12.14206044  -6.56057812 -15.23653087   1.73798644]\n",
      "\n",
      "\n",
      "After adding the dummy feature:\n",
      " [[  1.          -8.35364802]\n",
      " [  1.         -12.14206044]\n",
      " [  1.          -6.56057812]\n",
      " [  1.         -15.23653087]\n",
      " [  1.           1.73798644]]\n"
     ]
    }
   ],
   "source": [
    "print('Before adding the dummy feature:\\n', X_train[:5])\n",
    "print(\"\\n\")\n",
    "\n",
    "X_train_with_dummy = add_dummy_feature(X_train)\n",
    "print('After adding the dummy feature:\\n', X_train_with_dummy[:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878077fb",
   "metadata": {},
   "source": [
    "## C2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68853a68",
   "metadata": {},
   "source": [
    "### Quick Recap\n",
    "1. Training data contains features and label that is a real number\n",
    "2. Linear regression model uses linear combination of features to obtain output labels.In vectorized form Model or inference: $\\bf y=Xw$\n",
    "\n",
    "**Note**:\n",
    "- Model is paramterized by its weight vector.\n",
    "- It is described by its mathematical form and weight vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d6fea9",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "The general vectorized form is as follows:\n",
    "$$\\textbf{y}_{(n\\times1)}=\\textbf{X}_{n\\times(m+1)}\\textbf{w}_{(m+1)\\times1}$$\n",
    "where\n",
    "- $n$ is the number of examples in dataset(train/test/validation).\n",
    "- $m$ is the number of features\n",
    "- $\\bf X$ is a feature matrix which contain $(m+1)$ features for $n$ examples along rows. (Notice capital case bold **X** used for matrix)\n",
    "- $\\bf w$ is a weight vector containing $(m+1)$ weights one for each feature. (Notice the small case bold **w**)\n",
    "- $\\bf y$ is a label vector containing labels of $n$ examples with shape $(n,)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af92c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,w):\n",
    "    ''' Prediction of output label for a given icput.\n",
    "    Args:\n",
    "        X: Feature matrix of shape (n, m+1).\n",
    "        w: weight vector of shape (m+1, n).\n",
    "    Returns:\n",
    "        y: Predicted label vector of shape (n,).\n",
    "    '''\n",
    "    # Check to make sure that feature matrix and weight vector  have compatible shapes.\n",
    "    #print(X.shape,w.shape)\n",
    "    assert X.shape[-1] == w.shape[0]\n",
    "    return X @ w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9150556",
   "metadata": {},
   "source": [
    "We test this function with the following feature matrix $\\textbf{X}_{2\\times(3+1)}$\n",
    "$$\\textbf{X}_{2\\times4} = \\begin{bmatrix}\n",
    "    1 & 3 & 2 & 5 \\\\ 1 & 9 & 4 & 7\n",
    "\\end{bmatrix}$$\n",
    "and the weight vector $\\bf w$\n",
    "$$\\textbf{w}_{4\\times1} = \\begin{bmatrix}\n",
    "    1 \\\\ 1 \\\\ 1 \\\\ 1    \n",
    "\\end{bmatrix}$$\n",
    "Let's perform a matrix vector multiplication between feature matrix $\\bf X$ and the weight vector $\\bf w$ to obtain labels for all examples:\n",
    "$$\\begin{align*}\n",
    "    \\textbf{y} \n",
    "    &= \\textbf{Xw} \\\\\n",
    "    &= \\begin{bmatrix} 1 & 3 & 2 & 5 \\\\ 1 & 9 & 4 & 7 \\end{bmatrix} \\times \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\\\\n",
    "    &= \\begin{bmatrix} {1\\times1+3\\times1+2\\times1+5\\times1} \\\\ {1\\times1+9\\times1+4\\times1+7\\times1} \\end{bmatrix} \\\\\n",
    "    &= \\begin{bmatrix} 11 \\\\ 21 \\end{bmatrix}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f5e46dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_predict (__main__.TestPredict.test_predict)\n",
      "Test case predict function of linear regression ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 1.476s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f25431b25d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestPredict(unittest.TestCase):\n",
    "\n",
    "    def test_predict(self):\n",
    "        ''' Test case predict function of linear regression '''\n",
    "        # set up\n",
    "        train_matrix = cp.array([[1,3,2,5],[1,9,4,7]])\n",
    "        weight_vector = cp.array([1,1,1,1])\n",
    "        expected_label_vector = cp.array([11,21])\n",
    "\n",
    "        #call\n",
    "        predicted_label_vector = predict(train_matrix, weight_vector)\n",
    "\n",
    "        #asserts\n",
    "        #test the shape\n",
    "        self.assertEqual(predicted_label_vector.shape,(2,))\n",
    "\n",
    "        #and the contents\n",
    "        cp.testing.assert_array_equal(expected_label_vector,predicted_label_vector)\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestPredict', verbosity=2, exit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42867931",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Preparing the dataset'''\n",
    "[w0, w1] = [4, 3]\n",
    "n = 100\n",
    "x = 10 * cp.random.randn(n,)\n",
    "y = w0 + w1 * x + cp.random.randn(n, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a33400f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training feature matrix: (80, 2)\n",
      "Shape of training label vector: (80,)\n",
      "Shape of test feature matrix: (20, 2)\n",
      "Shape of test label matrix: (20,)\n"
     ]
    }
   ],
   "source": [
    "''' Preprocessing: Dummy feature and train-test-split'''\n",
    "X_with_dummy = add_dummy_feature(x)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_with_dummy, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Shape of training feature matrix:', X_train.shape)\n",
    "print('Shape of training label vector:', y_train.shape)\n",
    "\n",
    "print('Shape of test feature matrix:', X_test.shape)\n",
    "print('Shape of test label matrix:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72636392",
   "metadata": {},
   "source": [
    "Since we have not yet trained our model, let's use a random weight vector to get predictions from our model for a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de5b9c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29608942, 0.01114871])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = cp.random.rand(X_with_dummy.shape[1], )\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "743731b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = predict(X_train, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb4712",
   "metadata": {},
   "source": [
    "Let's compare the prediction with actual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5c4196e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.28233656, 0.26414283, 0.28942297, 0.33605589, 0.30312514,\n",
       "       0.46852799, 0.1767662 , 0.35930313, 0.3334066 , 0.3852154 ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2af215c",
   "metadata": {},
   "source": [
    "Actual labels are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "617239c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 25.46093108,  11.01006986, -22.5953873 ,  29.68261566,\n",
       "        13.9459229 , -12.48028136,  31.91874673,  16.10872934,\n",
       "       -51.59865948,  15.23210341])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6ac197",
   "metadata": {},
   "source": [
    "Since we use a random weight vector $\\bf w$ here, most of the predicted labels do not match the actual labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e91650",
   "metadata": {},
   "source": [
    "### Comparision of vectorized and non-vectorized version of inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6c14aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_vectorized_predict(X: cp.ndarray, w: cp.ndarray):\n",
    "    '''Prediction of output for a given icput.\n",
    "    Args:\n",
    "        X: Feature matrix of shape (n, m+1)\n",
    "        w: Weight vector of shape (m+1, n)\n",
    "    Returns:\n",
    "        y: Predicted label vector of shape (n, ).\n",
    "    '''\n",
    "    y = []\n",
    "    for i in range(0, X.shape[0]):\n",
    "        y_hat_i = 0\n",
    "        for j in range(0, X.shape[1]):\n",
    "            y_hat_i += X[i][j] * w[j]\n",
    "        y.append(y_hat_i)\n",
    "    return cp.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8954f9e2",
   "metadata": {},
   "source": [
    "Let's test this function with the same setup as vectorized implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f05a8f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_predict_non_vectorized (__main__.TestPredictNonVectorized.test_predict_non_vectorized)\n",
      "Test case predict function of linear regression ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.352s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f254319f230>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestPredictNonVectorized(unittest.TestCase):\n",
    "\n",
    "    def test_predict_non_vectorized(self):\n",
    "        '''Test case predict function of linear regression '''\n",
    "        #set up\n",
    "        train_matrix = cp.array([[1,3,2,5],[1,9,4,7]])\n",
    "        weight_vector = cp.array([1,1,1,1])\n",
    "        expected_label_vector = cp.array([11,21])\n",
    "\n",
    "        #call\n",
    "        predicted_label_vector = non_vectorized_predict(train_matrix,weight_vector)\n",
    "\n",
    "        #asserts\n",
    "        #test the shape\n",
    "        self.assertEqual(predicted_label_vector.shape, (2,))\n",
    "\n",
    "        #and its contents\n",
    "        cp.testing.assert_array_equal(expected_label_vector, predicted_label_vector)\n",
    "\n",
    "unittest.main(argv=[''],defaultTest='TestPredictNonVectorized', verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6101850c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time incurred in the vectorized inference is 0.000652552 s\n",
      "Total time incurred in the non-vectorized inference is 0.013366938 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "y_hat_vectorized = predict(X_train,w)\n",
    "end_time = time.time()\n",
    "print('Total time incurred in the vectorized inference is %0.9f s'%(end_time-start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "y_hat_non_vectorized = non_vectorized_predict(X_train,w)\n",
    "end_time = time.time()\n",
    "print('Total time incurred in the non-vectorized inference is %0.9f s'%(end_time-start_time))\n",
    "\n",
    "cp.testing.assert_array_equal(y_hat_vectorized,y_hat_non_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf5050ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "977.0\n"
     ]
    }
   ],
   "source": [
    "def loss(X: cp.ndarray, y: cp.ndarray, w: cp.ndarray):\n",
    "    error = predict(X, w) - y\n",
    "    return 0.5 * (cp.transpose(error) @ error)\n",
    "\n",
    "X = cp.array([[1, 2, 2, 1], [1, 1, 3, 2]])\n",
    "y = cp.array([3, 5])\n",
    "w = cp.array([1, 2, 3, 4]) * 2\n",
    "err = loss(X, y, w)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c82574b",
   "metadata": {},
   "source": [
    "#### Normal equation\n",
    "The weight vector is estimated by matrix multiplication of pseudo-inverse of feature matrix and label vector.\n",
    "\n",
    "The vectorized implementation is fairly straight forward.\n",
    "- We make use of ``cp.linalg.pinv`` for calculating pseudo inverse of the feature matrix\n",
    "\n",
    "The equation is \n",
    "$$\n",
    "\\text{w} = (X^TX)^{-1} X^T y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3350ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation(X, y):\n",
    "    '''Estimates parameters of the linear regression model with normal equation\n",
    "    Args:\n",
    "        X: feature matrix for given icputs.\n",
    "        y: Actual label vector\n",
    "\n",
    "    Returns:\n",
    "        Weight vector\n",
    "    '''\n",
    "    return cp.linalg.pinv(X) @ y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1686cc",
   "metadata": {},
   "source": [
    "We test this function with generated training set whose weight vector is known to us.\n",
    "- We setup the test with feature matrix, label vector and expected weight vectors\n",
    "- Next we estimate the weight vector with ``normal_equation`` function.\n",
    "- We test (a) shape and (ii) match between expected and estimated weight vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a68a594a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_normal_equation (__main__.TestNormalEquation.test_normal_equation)\n",
      "Test case for weight estimation for linear regression with normal equation method ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.574s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f254319e780>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestNormalEquation(unittest.TestCase):\n",
    "\n",
    "    def test_normal_equation(self):\n",
    "        '''\n",
    "        Test case for weight estimation for linear regression with normal equation method\n",
    "        '''\n",
    "\n",
    "        # set up\n",
    "        feature_matrix = X_train\n",
    "        label_vector = y_train\n",
    "        expected_weight_vector = cp.array([4.,3.])\n",
    "\n",
    "        #call\n",
    "        estimated_weight_vector = normal_equation(feature_matrix,label_vector)\n",
    "\n",
    "        #asserts\n",
    "        #test the shape\n",
    "        self.assertEqual(estimated_weight_vector.shape, (2,))\n",
    "\n",
    "        #and the contents\n",
    "        cp.testing.assert_array_almost_equal(estimated_weight_vector, expected_weight_vector,decimal=0)\n",
    "\n",
    "unittest.main(argv=[''],defaultTest='TestNormalEquation', verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bda8ea",
   "metadata": {},
   "source": [
    "### Gradient Descent(GD)\n",
    "GD is implemented as follows\n",
    "- Randomly initialize $w$ to 0.\n",
    "- Iterate until convergence:\n",
    "  - Calculate partial derivative of loss w.r.t weight vector\n",
    "  - Calculate new values of weights\n",
    "  - Update weights to new values simultaneously\n",
    "\n",
    "We use number of epochs as a convergence criteria in this implementation\n",
    "#### Partial derivative of loss function\n",
    "Let's first implement a function to calculate partial derivative of loss function, which is obtained with the following equation:\n",
    "$$\\bf \\frac{\\partial}{\\partial w}J(w) = X^T(Xw - y)$$\n",
    "The multiplication of transpose of feature matrix with the difference of predicted and actual label vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b701406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(X,y,w):\n",
    "    '''Calculate gradients of loss function w.r.t weight vector on training set\n",
    "    Arguments:\n",
    "        X: Feature matrix for training data\n",
    "        y: label vector of training data\n",
    "        w: Weight vector\n",
    "\n",
    "    Returns:\n",
    "        A vector of gradients\n",
    "    '''\n",
    "    return cp.transpose(X) @ (predict(X,w) - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c695652",
   "metadata": {},
   "source": [
    "The equation to update the weights would be:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\bf w_{\\text{new}} &= \\bf w_{\\text{old}} - \\alpha \\times \\frac{\\partial}{\\partial w}J(w) \\\\\n",
    "&= \\bf w_{\\text{old}} - \\alpha \\times X^T(Xw - y)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bba567b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(w, grad, lr):\n",
    "    ''' Updates the weights based on the gradient of the loss function.\n",
    "    Weight updates are carried out with the following formula:\n",
    "        w_new := w_old -lr*grad\n",
    "    Args:\n",
    "        1. w: weight vector\n",
    "        2. grad: gradient of loss w.r.t w\n",
    "        3. lr: learning rate\n",
    "    Returns:\n",
    "        Updated weight vector\n",
    "    '''\n",
    "    return (w - lr*grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "211e92e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X:cp.ndarray, y:cp.ndarray, lr:float, num_epochs:int):\n",
    "    '''Estimates parameters of linear regression model through gradient descent.\n",
    "    Args:\n",
    "        X: feature matrix for training data.\n",
    "        y: label vector for training data\n",
    "        lr: learning rate\n",
    "        num_epochs: Number of training steps\n",
    "    Returns:\n",
    "        Weight vector: Final weight vector\n",
    "        Error vector across different iterations\n",
    "        Weight vectors across different iterations\n",
    "    '''\n",
    "    w_all = [] # All parameters across different iterations\n",
    "    err_all = [] # All errors across different iterations\n",
    "    \n",
    "    # Parameter vector initialized to [0,0]\n",
    "    w = cp.zeros(X.shape[1])\n",
    "    # gradient descent loop\n",
    "    print()\n",
    "    for i in cp.arange(0,num_epochs):\n",
    "        w_all.append(w)\n",
    "        err_all.append(loss(X=X,w=w,y=y))\n",
    "        dJdw = calculate_gradient(X=X,y=y,w=w)\n",
    "        if (i%100) == 0:\n",
    "            print('Iteration #: %d, loss: %4.2f'%(i, err_all[-1]))\n",
    "        w = update_weights(w=w,grad=dJdw,lr=lr)\n",
    "    return w, err_all, w_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7f3dbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_gradient_descent (__main__.TestGradientDescent.test_gradient_descent)\n",
      "Test case for weight estimation for linear regression with gradient descent ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration #: 0, loss: 30368.98\n",
      "Iteration #: 100, loss: 169.50\n",
      "Iteration #: 200, loss: 68.13\n",
      "Iteration #: 300, loss: 47.57\n",
      "Iteration #: 400, loss: 43.40\n",
      "Iteration #: 500, loss: 42.56\n",
      "Iteration #: 600, loss: 42.38\n",
      "Iteration #: 700, loss: 42.35\n",
      "Iteration #: 800, loss: 42.34\n",
      "Iteration #: 900, loss: 42.34\n",
      "Iteration #: 1000, loss: 42.34\n",
      "Iteration #: 1100, loss: 42.34\n",
      "Iteration #: 1200, loss: 42.34\n",
      "Iteration #: 1300, loss: 42.34\n",
      "Iteration #: 1400, loss: 42.34\n",
      "Iteration #: 1500, loss: 42.34\n",
      "Iteration #: 1600, loss: 42.34\n",
      "Iteration #: 1700, loss: 42.34\n",
      "Iteration #: 1800, loss: 42.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 1.598s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #: 1900, loss: 42.34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f2798b655b0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestGradientDescent(unittest.TestCase):\n",
    "\n",
    "    def test_gradient_descent(self):\n",
    "        '''\n",
    "        Test case for weight estimation for linear regression with gradient descent\n",
    "        '''\n",
    "\n",
    "        #setup\n",
    "        feature_matrix = X_train\n",
    "        label_vector = y_train\n",
    "        expected_weights = cp.array([4.,3.])\n",
    "\n",
    "        #call\n",
    "        w, err_all, w_all = gradient_descent(\n",
    "            feature_matrix, label_vector, lr=0.0001, num_epochs=2000)\n",
    "        self.assertEqual(w.shape, (2,))\n",
    "        \n",
    "        cp.testing.assert_array_almost_equal(expected_weights, w, decimal=0)\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestGradientDescent',verbosity=2,exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2becabf",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent (MBGD)\n",
    "The key idea is to perform weight updates by computing gradient on batches of small number of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "761e8bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0, t1 = 200, 100000\n",
    "def learning_schedule(t):\n",
    "    return t0/(t+t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6634e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gd(X:cp.ndarray, y:cp.ndarray, num_iters:int, minibatch_size:int):\n",
    "    w_all = []\n",
    "    err_all = []\n",
    "\n",
    "    w = cp.zeros((X.shape[1]))\n",
    "    t = 0\n",
    "\n",
    "    for epoch in range(num_iters):\n",
    "        shuffled_indices = cp.random.permutation(X.shape[0])\n",
    "        X_shuffled = X[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            t += 1\n",
    "            xi = X_shuffled[i: i+minibatch_size]\n",
    "            yi = y_shuffled[i: i+minibatch_size]\n",
    "            #print(xi.shape[-1] == w.shape[0])\n",
    "            err_all.append(loss(xi,yi,w))\n",
    "            \n",
    "            gradients = 2/minibatch_size * calculate_gradient(xi, yi, w)\n",
    "            lr = learning_schedule(t)\n",
    "            w = update_weights(w, gradients, lr)\n",
    "            w_all.append(w)\n",
    "    return w, err_all, w_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "858bd2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_minibatch_gd (__main__.TestMiniBatchGradientDescent.test_minibatch_gd)\n",
      "Test case for weight estimation for linear regression with gradient descent ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 1.102s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f2550166360>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestMiniBatchGradientDescent(unittest.TestCase):\n",
    "\n",
    "    def test_minibatch_gd(self):\n",
    "        '''\n",
    "        Test case for weight estimation for linear regression with gradient descent\n",
    "        '''\n",
    "\n",
    "        #setup\n",
    "        feature_matrix = X_train\n",
    "        label_vector = y_train\n",
    "        expected_weights = cp.array([4.,3.])\n",
    "\n",
    "        #call\n",
    "        w, err_all, w_all = mini_batch_gd(\n",
    "            feature_matrix, label_vector, num_iters=200, minibatch_size=8)\n",
    "        self.assertEqual(w.shape, (2,))\n",
    "        \n",
    "        cp.testing.assert_array_almost_equal(expected_weights, w, decimal=0)\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestMiniBatchGradientDescent',verbosity=2,exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb5e72b",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)\n",
    "It is infact MBGD but with number of example per batch =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "31d0a48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X:cp.ndarray, y:cp.ndarray, num_epochs:int):\n",
    "    w_all = []\n",
    "    err_all = []\n",
    "\n",
    "    w = cp.zeros((X.shape[1]))\n",
    "    t=0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(X.shape[0]):\n",
    "            random_index = cp.random.randint(X.shape[0])\n",
    "            xi = X[random_index:random_index+1]\n",
    "            yi = y[random_index:random_index+1]\n",
    "            err_all.append(loss(xi, yi, w))\n",
    "\n",
    "            gradients = 2 * calculate_gradient(xi, yi, w)\n",
    "            lr = learning_schedule(epoch * X.shape[0] + i)\n",
    "            w = update_weights(w, gradients, lr)\n",
    "            w_all.append(w)\n",
    "    return w, err_all, w_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a3cdcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_sgd (__main__.TestStochasticGradientDescent.test_sgd)\n",
      "Test case for weight estimation for linear regression with gradient descent ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 16.422s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f2550167df0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestStochasticGradientDescent(unittest.TestCase):\n",
    "\n",
    "    def test_sgd(self):\n",
    "        '''\n",
    "        Test case for weight estimation for linear regression with gradient descent\n",
    "        '''\n",
    "\n",
    "        #setup\n",
    "        feature_matrix = X_train\n",
    "        label_vector = y_train\n",
    "        expected_weights = cp.array([4.,3.])\n",
    "\n",
    "        #call\n",
    "        w, err_all, w_all = sgd(\n",
    "            feature_matrix, label_vector, 200)\n",
    "        self.assertEqual(w.shape, (2,))\n",
    "        \n",
    "        cp.testing.assert_array_almost_equal(expected_weights, w, decimal=0)\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestStochasticGradientDescent',verbosity=2,exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486a6bd3",
   "metadata": {},
   "source": [
    "### Linear Regression: combining all components\n",
    "This part combines all the functions and components we implemented in the previous lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb956bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinReg(object):\n",
    "    '''\n",
    "    Linear Regression model\n",
    "    -----------------------\n",
    "    y = X @ w\n",
    "    X: A feature matrix\n",
    "    w: weight vector\n",
    "    y: label vector\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.t0 = 200\n",
    "        self.t1 = 100000\n",
    "    \n",
    "    def predict(self , X:cp.ndarray) -> cp.ndarray:\n",
    "        '''\n",
    "        Prediction of output label for a given icput.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for given icputs.\n",
    "        Returns:\n",
    "            y: Output vector as predicted by the given model.\n",
    "        '''\n",
    "        y = x @ self.w\n",
    "        return y\n",
    "    \n",
    "    def loss(self, X: cp.ndarray, y: cp.ndarray) -> float:\n",
    "        '''\n",
    "        Calculate the loss for a model based on known labels\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for given icputs.\n",
    "            y: Output label vector as predicted by the given model.\n",
    "        Returns:\n",
    "            Loss\n",
    "        '''\n",
    "\n",
    "        e = y - self.predict(X)\n",
    "        return 0.5* (cp.transpose(e)) @ e\n",
    "    \n",
    "    def rmse (self, X: cp.ndarray, y: cp.ndarray) -> float:\n",
    "        '''\n",
    "        Calculate the root mean squared error of predictions w.r.t actual label.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for given icputs.\n",
    "            y: Output label vector as predicted by the given model\n",
    "        Returns:\n",
    "            Loss\n",
    "        '''\n",
    "        return cp.sqrt((2/X.shape[0])) * self.loss(X,y)\n",
    "    \n",
    "\n",
    "    def fit (self, X: cp.ndarray, y: cp.ndarray) -> cp.ndarray:\n",
    "        '''\n",
    "        Estimates the parameters of the linear regression model with normal equation\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for given icputs.\n",
    "            y: Output label vector as predicted by the given model\n",
    "        Returns:\n",
    "            weight vector\n",
    "        '''\n",
    "        self.w = cp.linalg.pinv(X) @ y\n",
    "        return self.w\n",
    "    \n",
    "\n",
    "    def calculate_gradient(self, X: cp.ndarray, y: cp.ndarray) -> cp.ndarray:\n",
    "        '''\n",
    "        Calculates gradients of loss function w.r.t weight vector on training set.\n",
    "        Args:\n",
    "            X: Feature matrix for given icputs.\n",
    "            y: Output label vector as predicted by the given model\n",
    "        Returns:\n",
    "            a vector of gradients.\n",
    "        '''\n",
    "        return cp.transpose(X) @ (self.predict(X) - y)\n",
    "    \n",
    "    def update_weights (self, grad:cp.ndarray, lr: float) -> cp.ndarray:\n",
    "        '''\n",
    "        Updates the weights based on the gradient of the loss function.\n",
    "\n",
    "        Weight updates are carried out with the following formula:\n",
    "            w_new := w_old - lr * grad\n",
    "        Args:\n",
    "            1. w: weight vector\n",
    "            2. grad: gradient of loss w.r.t w\n",
    "            3. lr: learning rate\n",
    "        Returns:\n",
    "            Updated weight vector\n",
    "        '''\n",
    "        return (self.w - lr * grad)\n",
    "\n",
    "    def learning_schedule(self, t):\n",
    "        return self.t0 / (t + self.t1)\n",
    "    \n",
    "    def gd(self, X: cp.ndarray, y:cp.ndarray, num_epochs:int, lr:float) -> cp.ndarray:\n",
    "        '''\n",
    "        Estimates the parameters of linear regression model through gradient descent\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for training data\n",
    "            y: Label vector for training data\n",
    "            num_epochs: Number of training steps\n",
    "            lr: learning rate\n",
    "\n",
    "        Returns:\n",
    "            Weight vector: Final weight vector\n",
    "        '''\n",
    "        self.w = cp.zeros((X.shape[1]))\n",
    "        self.w_all = []\n",
    "        self.err_all = []\n",
    "        for i in cp.arange(0,num_epochs):\n",
    "            dJdw = self.calculate_gradient(X, y)\n",
    "            self.w_all.append(self.w)\n",
    "            self.err_all.append(self.loss(X,y))\n",
    "            self.w = self.update_weights(dJdw, lr)\n",
    "        return self.w\n",
    "    \n",
    "    def mbgd (self, X:cp.ndarray, y:cp.ndarray, num_epochs:int, batch_size:int) -> cp.ndarray:\n",
    "        '''\n",
    "        Estimates the parameters of linear regression model through gradient descent\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for training data\n",
    "            y: Label vector for training data\n",
    "            num_epochs: Number of training steps\n",
    "            batch_size: Number of examples in a batch\n",
    "\n",
    "        Returns:\n",
    "            Weight vector: Final weight vector\n",
    "        '''\n",
    "        self.w = cp.zeros((X.shape[1]))\n",
    "        self.w_all = []\n",
    "        self.err_all = []\n",
    "        mini_batch_id = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            shuffled_indices = cp.random.permutation(X.shap[0])\n",
    "            X_shuffled = X[shuffled_indices]\n",
    "            y_shuffled = y[shuffled_indices]\n",
    "\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                mini_batch_id += 1\n",
    "                xi = X_shuffled[i: i+batch_size]\n",
    "                yi = y_shuffled[i: i+batch_size]\n",
    "                self.w_all.append(self.w)\n",
    "                self.err_all.append(self.loss(xi, yi))\n",
    "\n",
    "                dJdw = 2/batch_size * self.calculate_gradient(xi, yi)\n",
    "                self.w = self.update_weights(dJdw, self.learning_schedule(mini_batch_id))\n",
    "            \n",
    "        return self.w\n",
    "    \n",
    "    def sgd (self, X: cp.ndarray, y:cp.ndarray, num_epochs:int) -> cp.ndarray:\n",
    "        '''\n",
    "        Estimates the parameters of linear regression model through gradient descent\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for training data\n",
    "            y: Label vector for training data\n",
    "            num_epochs: Number of training steps\n",
    "            batch_size: Number of examples in a batch\n",
    "\n",
    "        Returns:\n",
    "            Weight vector: Final weight vector\n",
    "        '''\n",
    "        self.w = cp.zeros((X.shape[1]))\n",
    "        self.w_all = []\n",
    "        self.err_all = []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            for i in range(X.shape[0]):\n",
    "                random_index = cp.random.randint(X.shape[0])\n",
    "                xi = X[random_index:random_index+1]\n",
    "                yi = y[random_index:random_index+1]\n",
    "\n",
    "                self.w_all.append(self.w)\n",
    "                self.err_all.append(self.loss(xi,yi))\n",
    "\n",
    "                gradients = 2 * self.calculate_gradient(xi, yi)\n",
    "                lr = self.learning_schedule(epoch * X.shape[0] +i)\n",
    "                self.w = self.update_weights(gradients, lr)\n",
    "        \n",
    "        return self.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5bb75d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
