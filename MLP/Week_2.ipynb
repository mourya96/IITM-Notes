{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preprocessing Techniques**\n",
    "Data preprocessing involves several transformations that are applied to the raw data and make it more amenable for learning. It is carried out before using it for model training or prediction.\n",
    "\n",
    "There are many pre-processing techniques for\n",
    "- Data cleaning\n",
    "  - Data imputation\n",
    "  - Feature scaling\n",
    "- Feature transformation\n",
    "  - Polynomial features\n",
    "  - Discretization\n",
    "  - Handling categorical features\n",
    "  - Custom Transformers\n",
    "  - Composite Transformers\n",
    "    - Apply transformation of diverse features\n",
    "    - TargetTransformedRegressor\n",
    "- Feature Selection\n",
    "  - Filter based feature selection\n",
    "  - Wrapper based feature selection\n",
    "- Feature Extraction\n",
    "  - PCA\n",
    "\n",
    "The transformations are applied in a specific order and the order can be specified via ``Pipeline``. We need to apply different transformations based on the feature type. ``FeatureUnion`` helps us perform that task and combine outputs from multiple transformations into a single transformed feature matrix. We will also study how to visualize this pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DictVectorizer\n",
    "\n",
    "Many a times the data is present as a **list of dictionary objects**. ML algorithms expect the data to be in **matrix form** of shape $(n,m)$ where $n$ is the number of samples and $m$ is the number of features.\n",
    "\n",
    "``DictVectorizer`` **converts** a *list of dictionary objects to feature matrix*.\n",
    "\n",
    "Let's create a sample data for demo purpose containing ``age`` and ``height`` of children\n",
    "> Each record/sample in dictionary with two keys ``age`` and ``height``, and their corresponding values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{'age' : 4, 'height' : 96.0},\n",
    "        {'age' : 1, 'height' : 73.9},\n",
    "        {'age' : 3, 'height' : 88.9},\n",
    "        {'age' : 2, 'height' : 81.6}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are 4 data samples with 2 features each\n",
    "\n",
    "Let's make use of ``DictVectorizer`` to convert the list of dictionary objects to the feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4. , 96. ],\n",
       "       [ 1. , 73.9],\n",
       "       [ 3. , 88.9],\n",
       "       [ 2. , 81.6]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dv = DictVectorizer(sparse = False)\n",
    "data_transformed = dv.fit_transform(data)\n",
    "data_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformed data is in the feature matrix form- 4 examples with 2 features each i.e shape $(4,2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Data Imputation**\n",
    "- Many machine learning algorithms need full feature matrix and they may not work in the presence of missing data\n",
    "- Data imputation identified **missing values** in each feature of the dataset and **replaces** them with an **appropriate value** based on **fixed strategy** such as:\n",
    "  - **mean** or **median** or **mode** of that feature.\n",
    "  - **use specified constant** value\n",
    "\n",
    "Sklearn library provides ``sklearn.impute.SimpleImputer`` class for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of its important parameters:\n",
    "- *missing_values*: Could be ``int, float, str, np.nan`` or ``None``. By default its ``np.nan``.\n",
    "- *strategy*: default is 'mean'. One of the following strategies can be used.   \n",
    "  - ``mean``- missing values are replaced using the **mean** along each column\n",
    "  - ``median`` - missing values are replaced using the **median** along each column\n",
    "  - ``most_frequent`` - missing values are replaced using the **most frequent** along each column\n",
    "  - ``constant`` - missing values are replaced with values specified in ``fill_value`` argument.\n",
    "- ``add_indicator`` is a boolean parameter that when set to ``True`` returns **missing value indicators** in ``indicators_`` member variable.\n",
    "\n",
    "**Note**:\n",
    "- ``mean`` and ``mode`` strategies can only be used with numeric data.\n",
    "- ``most_frequent`` and ``constant`` strategies can be used with strings or numeric data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data imputation on real world dataset\n",
    "Let's perform data imputation on real world dataset. We will be using [heart-disease from uci machine learning repository](https://archive.ics.uci.edu/ml/datasets/Heart+Disease) for this purpose. We will load this dataset from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','num']\n",
    "heart_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data',header=None,names=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1.**: Check if dataset contains missing values.\n",
    "- This can be checked via dataset description or by check number of ``nan`` or ``np.null`` in the dataframe. Howevver such check can be performed only for numerical features.\n",
    "- For non-numerical features, we can list their unique values and check if there are values like ``?``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    float64\n",
      " 1   sex       303 non-null    float64\n",
      " 2   cp        303 non-null    float64\n",
      " 3   trestbps  303 non-null    float64\n",
      " 4   chol      303 non-null    float64\n",
      " 5   fbs       303 non-null    float64\n",
      " 6   restecg   303 non-null    float64\n",
      " 7   thalach   303 non-null    float64\n",
      " 8   exang     303 non-null    float64\n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    float64\n",
      " 11  ca        303 non-null    object \n",
      " 12  thal      303 non-null    object \n",
      " 13  num       303 non-null    int64  \n",
      "dtypes: float64(11), int64(1), object(2)\n",
      "memory usage: 33.3+ KB\n"
     ]
    }
   ],
   "source": [
    "heart_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if there are missing values in numerical columns - here we have checked it for all columns in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "cp          0\n",
       "trestbps    0\n",
       "chol        0\n",
       "fbs         0\n",
       "restecg     0\n",
       "thalach     0\n",
       "exang       0\n",
       "oldpeak     0\n",
       "slope       0\n",
       "ca          0\n",
       "thal        0\n",
       "num         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(heart_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two non-numerical features: ``ca`` and ``thal``.\n",
    "- List their unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in ca: ['0.0' '3.0' '2.0' '1.0' '?']\n",
      "Unique values in thal: ['6.0' '3.0' '7.0' '?']\n"
     ]
    }
   ],
   "source": [
    "print('Unique values in ca:', heart_data.ca.unique())\n",
    "print('Unique values in thal:', heart_data.thal.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of them contain ``?`` which is a missing values. Let's count the number of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# missing values in ca: 4\n",
      "# missing values in thal: 2\n"
     ]
    }
   ],
   "source": [
    "print('# missing values in ca:', heart_data.loc[heart_data.ca == '?','ca'].count())\n",
    "print('# missing values in thal:', heart_data.loc[heart_data.thal ==\"?\",'thal'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2**: Replace '?' with ``nan``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_data.replace('?',np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3**: Fill the missing values with ``sklearn`` missing value imputation utilities.\n",
    "> Here we use ``SimpleImputer`` with ``mean`` strategy.\n",
    "\n",
    "We will try two variations- \n",
    "- ``add_indicator = False``: Default choice that only imputes missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    }
   ],
   "source": [
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer = imputer.fit(heart_data)\n",
    "heart_data_imputed = imputer.transform(heart_data)\n",
    "print(heart_data_imputed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ``add_indicator = True``: Adds additional column for each column containing missing values. In this case it adds two column, one for ``ca`` and the other for ``thal``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 16)\n"
     ]
    }
   ],
   "source": [
    "imputer = SimpleImputer(missing_values= np.nan, strategy='mean', add_indicator=True)\n",
    "imputer = imputer.fit(heart_data)\n",
    "heart_data_imputed_with_indicator = imputer.transform(heart_data)\n",
    "print(heart_data_imputed_with_indicator.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Feature Scaling**\n",
    "\n",
    "Feature scaling **transforms feature values** such that **all the features are on the same scale**.\n",
    "When we use feature matrix with all the features on the same scale.\n",
    "- **Enables faster convergence** in iterative optimization algorithms like gradient descent and its variants.\n",
    "- The performance of ML algorithms such as SVM, K-NN and K-means etc, that compute euclidean distance among input samples gets impacted if the features are not scaled.\n",
    "\n",
    "Tree based ML algorithms are not affected by feature-scaling. In other words, feature scaling is not required for tree based ML algorithms\n",
    "\n",
    "Feature scaling can be performed with the following methods:\n",
    "- Standardization\n",
    "- Normalization\n",
    "- MaxAbsScaler.\n",
    "\n",
    "Let's demonstrate feature scaling on real world dataset. For this purpose, we will be using [abalone dataset](https://archive.ics.uci.edu/ml/datasets/abalone). We will use different scaling utilities in ``sklearn`` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Sex','Length','Diameter','Height','Whole weight','Shucked weight','Viscera weight','Shell weight','Rings']\n",
    "abalone_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data',header=None,names=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1**: Examine the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4177 entries, 0 to 4176\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Sex             4177 non-null   object \n",
      " 1   Length          4177 non-null   float64\n",
      " 2   Diameter        4177 non-null   float64\n",
      " 3   Height          4177 non-null   float64\n",
      " 4   Whole weight    4177 non-null   float64\n",
      " 5   Shucked weight  4177 non-null   float64\n",
      " 6   Viscera weight  4177 non-null   float64\n",
      " 7   Shell weight    4177 non-null   float64\n",
      " 8   Rings           4177 non-null   int64  \n",
      "dtypes: float64(7), int64(1), object(1)\n",
      "memory usage: 293.8+ KB\n"
     ]
    }
   ],
   "source": [
    "abalone_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1a**: [Optional]: convert non-numerical attributes into numerical ones\n",
    "> In this dataset only ``Sex`` is the non-numeric column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['M', 'F', 'I'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_data.Sex.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4177 entries, 0 to 4176\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Sex             4177 non-null   int64  \n",
      " 1   Length          4177 non-null   float64\n",
      " 2   Diameter        4177 non-null   float64\n",
      " 3   Height          4177 non-null   float64\n",
      " 4   Whole weight    4177 non-null   float64\n",
      " 5   Shucked weight  4177 non-null   float64\n",
      " 6   Viscera weight  4177 non-null   float64\n",
      " 7   Shell weight    4177 non-null   float64\n",
      " 8   Rings           4177 non-null   int64  \n",
      "dtypes: float64(7), int64(2)\n",
      "memory usage: 293.8 KB\n"
     ]
    }
   ],
   "source": [
    "#Assign numeric values to sex.\n",
    "abalone_data = abalone_data.replace({'Sex': {'M':1,'F':2,'I':3}})\n",
    "abalone_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2**: Separate labels from features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe object after deleting the column\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4177 entries, 0 to 4176\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Sex             4177 non-null   int64  \n",
      " 1   Length          4177 non-null   float64\n",
      " 2   Diameter        4177 non-null   float64\n",
      " 3   Height          4177 non-null   float64\n",
      " 4   Whole weight    4177 non-null   float64\n",
      " 5   Shucked weight  4177 non-null   float64\n",
      " 6   Viscera weight  4177 non-null   float64\n",
      " 7   Shell weight    4177 non-null   float64\n",
      "dtypes: float64(7), int64(1)\n",
      "memory usage: 261.2 KB\n"
     ]
    }
   ],
   "source": [
    "y = abalone_data.pop('Rings')\n",
    "print('The dataframe object after deleting the column')\n",
    "abalone_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3**: Examing the feature scales\n",
    "\n",
    "#### Statistical method\n",
    "Check the scales of different features with ``describe()`` method of dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>4177.0</td>\n",
       "      <td>1.955470</td>\n",
       "      <td>0.827815</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Length</th>\n",
       "      <td>4177.0</td>\n",
       "      <td>0.523992</td>\n",
       "      <td>0.120093</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>0.5450</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.8150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diameter</th>\n",
       "      <td>4177.0</td>\n",
       "      <td>0.407881</td>\n",
       "      <td>0.099240</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.4250</td>\n",
       "      <td>0.480</td>\n",
       "      <td>0.6500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Height</th>\n",
       "      <td>4177.0</td>\n",
       "      <td>0.139516</td>\n",
       "      <td>0.041827</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.165</td>\n",
       "      <td>1.1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Whole weight</th>\n",
       "      <td>4177.0</td>\n",
       "      <td>0.828742</td>\n",
       "      <td>0.490389</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.4415</td>\n",
       "      <td>0.7995</td>\n",
       "      <td>1.153</td>\n",
       "      <td>2.8255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shucked weight</th>\n",
       "      <td>4177.0</td>\n",
       "      <td>0.359367</td>\n",
       "      <td>0.221963</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.3360</td>\n",
       "      <td>0.502</td>\n",
       "      <td>1.4880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Viscera weight</th>\n",
       "      <td>4177.0</td>\n",
       "      <td>0.180594</td>\n",
       "      <td>0.109614</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0935</td>\n",
       "      <td>0.1710</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.7600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shell weight</th>\n",
       "      <td>4177.0</td>\n",
       "      <td>0.238831</td>\n",
       "      <td>0.139203</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.1300</td>\n",
       "      <td>0.2340</td>\n",
       "      <td>0.329</td>\n",
       "      <td>1.0050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 count      mean       std     min     25%     50%    75%  \\\n",
       "Sex             4177.0  1.955470  0.827815  1.0000  1.0000  2.0000  3.000   \n",
       "Length          4177.0  0.523992  0.120093  0.0750  0.4500  0.5450  0.615   \n",
       "Diameter        4177.0  0.407881  0.099240  0.0550  0.3500  0.4250  0.480   \n",
       "Height          4177.0  0.139516  0.041827  0.0000  0.1150  0.1400  0.165   \n",
       "Whole weight    4177.0  0.828742  0.490389  0.0020  0.4415  0.7995  1.153   \n",
       "Shucked weight  4177.0  0.359367  0.221963  0.0010  0.1860  0.3360  0.502   \n",
       "Viscera weight  4177.0  0.180594  0.109614  0.0005  0.0935  0.1710  0.253   \n",
       "Shell weight    4177.0  0.238831  0.139203  0.0015  0.1300  0.2340  0.329   \n",
       "\n",
       "                   max  \n",
       "Sex             3.0000  \n",
       "Length          0.8150  \n",
       "Diameter        0.6500  \n",
       "Height          1.1300  \n",
       "Whole weight    2.8255  \n",
       "Shucked weight  1.4880  \n",
       "Viscera weight  0.7600  \n",
       "Shell weight    1.0050  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9244b6adea22edad6e19cdea93c196ea7ddff3c1d91dfb077ea542e13d85dd05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
